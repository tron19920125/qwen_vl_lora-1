{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4f6649",
   "metadata": {},
   "source": [
    "# 1. Azure ML 前置准备\n",
    "\n",
    "本章节涵盖 Azure ML 工作区的创建与连接、本地环境配置等前置准备工作。\n",
    "\n",
    "- **1.1** 安装本地依赖\n",
    "- **1.2** 创建 Azure ML 工作区(如已有工作区可跳过)\n",
    "- **1.3** 连接 Azure ML 工作区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22b523",
   "metadata": {},
   "source": [
    "# 端到端：Azure VM 准备 + Azure ML 上使用 LoRA 微调 Qwen3-VL-4B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03860939",
   "metadata": {},
   "source": [
    "## 1.1 安装本地依赖(仅 Notebook 运行环境)\n",
    "\n",
    "- 本节为你的本机/Notebook 内核安装最小依赖,便于连接 Azure ML、做数据预处理与可视化。\n",
    "- 训练将在 Azure ML 计算上执行,本机不安装 torch/transformers 等大依赖。\n",
    "- 若你在中国主权云(设置 AZURE_CLOUD_NAME=AzureChinaCloud),安装命令会自动切换镜像源。\n",
    "- 如已有满足条件的环境,可跳过本节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4287f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements-notebook.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements-notebook.txt\n",
    "# 最小依赖（Notebook 端）\n",
    "# 训练发生在 Azure ML 计算上，本地无需安装 torch/transformers 等大依赖\n",
    "azure-ai-ml>=1.15.0\n",
    "azure-identity>=1.16.0\n",
    "pandas>=2.1.0\n",
    "pyarrow>=15.0.0\n",
    "pillow>=10.2.0\n",
    "tqdm>=4.66.0\n",
    "ipywidgets>=8.1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafac309",
   "metadata": {},
   "source": [
    "## 1.3 连接 Azure ML 工作区\n",
    "\n",
    "完成工作区创建(或使用已有工作区)后,在本地 Notebook 环境中通过 Python SDK 连接到 Azure ML 工作区。\n",
    "\n",
    "以下示例代码展示如何使用 `InteractiveBrowserCredential` 进行身份验证并创建 `MLClient` 对象:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvvhpkgm5bc",
   "metadata": {},
   "source": [
    "## 1.2 创建 Azure Machine Learning 工作区\n",
    "\n",
    "本节将指导你在 Azure 门户上创建 Azure Machine Learning 工作区,这是运行 Azure ML 训练作业的前置条件。\n",
    "\n",
    "**如果你已有工作区,可跳过本节,直接进入 1.3 连接工作区。**\n",
    "\n",
    "### 1.2.1 前置条件\n",
    "- Azure 订阅具备创建 Azure ML 工作区的权限。\n",
    "- 本地已安装 Azure CLI 或能够登录 Azure 门户。\n",
    "- 已规划好资源组名称和工作区所在区域。\n",
    "\n",
    "### 1.2.2 登录 Azure 门户\n",
    "\n",
    "根据你的云环境选择对应的门户:\n",
    "- **公有云**: [https://portal.azure.com/](https://portal.azure.com/)\n",
    "- **中国区**: [https://portal.azure.cn/](https://portal.azure.cn/)\n",
    "\n",
    "使用你的 Azure 账号登录。\n",
    "\n",
    "### 1.2.3 创建 Azure Machine Learning 工作区\n",
    "\n",
    "#### 进入创建向导\n",
    "1. 在 Azure 门户主页,点击左上角的 **\"创建资源\"** 按钮。\n",
    "   - ![创建资源入口](images/azure_ml_create_resource.png)\n",
    "2. 在搜索框中输入 **\"Machine Learning\"**,选择 **\"Azure Machine Learning\"**。\n",
    "   - ![搜索 ML 服务](images/azure_ml_search.png)\n",
    "3. 点击 **\"创建\"** 按钮,进入工作区创建向导。\n",
    "   - ![开始创建](images/azure_ml_create_start.png)\n",
    "\n",
    "#### 基本信息配置\n",
    "在 **\"基本信息\"** 页签填写以下内容:\n",
    "\n",
    "| 字段 | 说明 | 示例值 |\n",
    "|------|------|--------|\n",
    "| **订阅** | 选择你的 Azure 订阅 | `Pay-As-You-Go` |\n",
    "| **资源组** | 选择现有资源组或创建新资源组 | `OctWorkRG` |\n",
    "| **工作区名称** | 工作区的唯一名称(3-33字符,字母数字和连字符) | `ml-cn3-01` |\n",
    "| **区域** | 选择工作区部署的 Azure 区域 | `chinanorth3` 或 `eastus` |\n",
    "| **存储账户** | 默认新建,或选择现有存储账户 | 默认自动创建 |\n",
    "| **Key Vault** | 默认新建,用于存储密钥和凭据 | 默认自动创建 |\n",
    "| **Application Insights** | 默认新建,用于监控和日志 | 默认自动创建 |\n",
    "| **容器注册表** | 可选,用于存储 Docker 镜像 | 首次可留空 |\n",
    "\n",
    "![基本信息配置](images/azure_ml_basics.png)\n",
    "\n",
    "**重要提示**:\n",
    "- **区域选择**: 确保选择的区域支持你后续需要的 GPU VM SKU(如 NVads A10 v5)。\n",
    "- **资源组**: 建议将相关资源放在同一资源组,便于管理和成本跟踪。\n",
    "\n",
    "#### 网络配置(可选)\n",
    "在 **\"网络\"** 页签可以配置工作区的网络访问方式:\n",
    "- **公共终结点(所有网络)**: 默认选项,允许从任何位置访问。\n",
    "- **专用终结点**: 通过虚拟网络私密访问,适合企业安全要求。\n",
    "\n",
    "对于开发和学习场景,使用默认的 **\"公共终结点(所有网络)\"** 即可。\n",
    "\n",
    "![网络配置](images/azure_ml_network.png)\n",
    "\n",
    "#### 高级配置(可选)\n",
    "在 **\"高级\"** 页签可以配置:\n",
    "- **数据加密**: 选择使用 Microsoft 托管密钥还是客户托管密钥。\n",
    "- **标识**: 配置系统分配或用户分配的托管标识。\n",
    "- **标签**: 为资源添加标签,便于分类和计费跟踪。\n",
    "\n",
    "对于初次使用,使用默认配置即可。\n",
    "\n",
    "![高级配置](images/azure_ml_advanced.png)\n",
    "\n",
    "#### 检查并创建\n",
    "1. 点击 **\"检查 + 创建\"** 按钮。\n",
    "   - ![检查配置](images/azure_ml_review.png)\n",
    "2. Azure 会验证你的配置,确认所有字段都正确填写且符合规则。\n",
    "3. 检查配置摘要,确认无误后点击 **\"创建\"** 按钮。\n",
    "   - ![确认创建](images/azure_ml_create_confirm.png)\n",
    "4. 部署过程通常需要 **2-5 分钟**,可以在通知面板查看进度。\n",
    "   - ![部署进行中](images/azure_ml_deployment.png)\n",
    "\n",
    "### 1.2.4 验证工作区创建成功\n",
    "\n",
    "#### 查看部署结果\n",
    "1. 部署完成后,点击 **\"转到资源\"** 按钮。\n",
    "   - ![部署完成](images/azure_ml_deployment_complete.png)\n",
    "2. 进入工作区概览页面,确认以下信息:\n",
    "   - 工作区名称、资源组、订阅 ID\n",
    "   - 工作区 URL(用于访问 Azure ML Studio)\n",
    "   - 关联的存储账户、Key Vault 等资源\n",
    "\n",
    "![工作区概览](images/azure_ml_workspace_overview.png)\n",
    "\n",
    "#### 访问 Azure ML Studio\n",
    "1. 在工作区概览页点击 **\"启动工作室\"** 按钮,或直接访问:\n",
    "   - **公有云**: [https://ml.azure.com/](https://ml.azure.com/)\n",
    "   - **中国区**: [https://ml.azure.cn/](https://ml.azure.cn/)\n",
    "2. 选择你创建的工作区,进入 Azure ML Studio 管理界面。\n",
    "   - ![ML Studio 首页](images/azure_ml_studio.png)\n",
    "\n",
    "### 1.2.5 创建计算集群(可选,也可通过代码创建)\n",
    "\n",
    "如果你想通过门户创建计算集群,可以按以下步骤操作:\n",
    "\n",
    "#### 进入计算页面\n",
    "1. 在 Azure ML Studio 左侧导航栏,点击 **\"计算\"**。\n",
    "2. 选择 **\"计算群集\"** 选项卡。\n",
    "3. 点击 **\"+ 新建\"** 按钮。\n",
    "   - ![创建计算集群](images/azure_ml_aks_create.png)\n",
    "\n",
    "#### 配置计算集群\n",
    "填写以下配置:\n",
    "\n",
    "| 字段 | 说明 | 示例值 |\n",
    "|------|------|--------|\n",
    "| **计算名称** | 集群的唯一名称 | `gpu-cluster` |\n",
    "| **虚拟机大小** | 选择 VM SKU(支持 GPU 的节点) | `Standard_NVads_A10_v5` |\n",
    "| **最小节点数** | 最小节点数量(0表示空闲时自动缩减) | `0` |\n",
    "| **最大节点数** | 最大节点数量 | `4` |\n",
    "| **空闲时间** | 节点空闲多久后自动缩减 | `120` 秒 |\n",
    "\n",
    "![配置计算集群](images/azure_ml_aks_config.png)\n",
    "\n",
    "**重要提示**:\n",
    "- **成本控制**: 设置最小节点数为0,空闲时自动缩减,避免不必要的费用。\n",
    "- **GPU 支持**: 确保选择的 VM SKU 支持 GPU(NC、NV 系列)。\n",
    "\n",
    "#### 完成创建\n",
    "1. 点击 **\"创建\"** 按钮。\n",
    "2. 计算集群创建通常需要 **5-10 分钟**,完成后状态变为 **\"成功\"**。\n",
    "\n",
    "![计算集群列表](images/azure_ml_aks_list.png)\n",
    "\n",
    "### 1.2.6 获取工作区连接信息\n",
    "\n",
    "完成工作区创建后,你需要以下信息来通过 Python SDK 连接工作区:\n",
    "\n",
    "#### 通过门户获取\n",
    "在工作区概览页面可以找到:\n",
    "- **订阅 ID**: 在 \"基本信息\" 部分\n",
    "- **资源组**: 在 \"基本信息\" 部分\n",
    "- **工作区名称**: 页面顶部标题\n",
    "\n",
    "![获取连接信息](images/azure_ml_connection_info.png)\n",
    "\n",
    "#### 通过 Azure CLI 获取\n",
    "```bash\n",
    "# 列出订阅\n",
    "az account list --output table\n",
    "\n",
    "# 列出资源组\n",
    "az group list --output table\n",
    "\n",
    "# 列出工作区\n",
    "az ml workspace list --resource-group <资源组名称> --output table\n",
    "\n",
    "# 显示工作区详情\n",
    "az ml workspace show --name <工作区名称> --resource-group <资源组名称>\n",
    "```\n",
    "\n",
    "记录这些信息,将在下一节 **1.3 连接工作区** 中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9531065",
   "metadata": {},
   "source": [
    "# 2. 正式操作\n",
    "\n",
    "前置准备完成后,本章节将完成以下操作:\n",
    "- 通过 Azure SDK 交互式登录并连接工作区\n",
    "- 准备数据集并注册到 Azure ML\n",
    "- 在 Azure ML 计算集群上运行 LLaMA-Factory 训练作业\n",
    "- 导出合并模型并注册为模型资产"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8uqv9qg3jr",
   "metadata": {},
   "source": [
    "## 2.1 交互式登录并连接工作区\n",
    "\n",
    "使用 Azure SDK 进行交互式身份验证,并连接到你在第 1 章创建或准备好的 Azure ML 工作区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09265db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to workspace: aml-hu-east-west-us2\n",
      "Location: westus2\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import InteractiveBrowserCredential, AzureAuthorityHosts\n",
    "from azure.ai.ml import MLClient\n",
    "import os\n",
    "\n",
    "# 基础配置（可使用环境变量覆盖）\n",
    "# SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\", \"3e859a28-17f7-420e-bc02-624301a676f7\")\n",
    "# RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\", \"OctWorkRG\")\n",
    "# WORKSPACE_NAME = os.getenv(\"AZUREML_WORKSPACE_NAME\", \"ml-cn3-01\")\n",
    "SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\", \"7a03e9b8-18d6-48e7-b186-0ec68da9e86f\")\n",
    "RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\", \"aml-rg\")\n",
    "WORKSPACE_NAME = os.getenv(\"AZUREML_WORKSPACE_NAME\", \"aml-hu-east-west-us2\")\n",
    "\n",
    "# 切换公有云/主权云（AzureChinaCloud）\n",
    "USE_CHINA_CLOUD = False\n",
    "authority_host = AzureAuthorityHosts.AZURE_CHINA if USE_CHINA_CLOUD else AzureAuthorityHosts.AZURE_PUBLIC_CLOUD\n",
    "ml_client_kwargs = {\"cloud\": \"AzureChinaCloud\"} if USE_CHINA_CLOUD else {}\n",
    "\n",
    "# 交互式登录\n",
    "credential = InteractiveBrowserCredential(authority=authority_host,tenant_id=\"16b3c013-d300-468d-ac64-7eda0820b6d3\")\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    "    **ml_client_kwargs,\n",
    ")\n",
    "\n",
    "print(f\"Connected to workspace: {ml_client.workspace_name}\")\n",
    "workspace = ml_client.workspaces.get(ml_client.workspace_name)\n",
    "print(f\"Location: {workspace.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257022d",
   "metadata": {},
   "source": [
    "## 2.2 准备目录结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e791e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace_dir: /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl\n",
      "src_dir: /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/src\n",
      "datasets_dir: /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/datasets\n",
      "config_dir: /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/config\n",
      "outputs_dir: /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/outputs\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "workspace_dir = pathlib.Path.cwd() / \"workshop_qwen_vl\"\n",
    "src_dir = workspace_dir / \"src\"\n",
    "env_dir = workspace_dir / \"env\"\n",
    "datasets_dir = workspace_dir / \"datasets\"\n",
    "config_dir = workspace_dir / \"config\"\n",
    "outputs_dir = workspace_dir / \"outputs\"\n",
    "\n",
    "for d in [workspace_dir, src_dir, env_dir, datasets_dir, config_dir, outputs_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"workspace_dir:\", workspace_dir)\n",
    "print(\"src_dir:\", src_dir)\n",
    "print(\"datasets_dir:\", datasets_dir)\n",
    "print(\"config_dir:\", config_dir)\n",
    "print(\"outputs_dir:\", outputs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f8dad",
   "metadata": {},
   "source": [
    "## 2.3 下载并处理 vqav2-small 数据集为 LLaMA-Factory JSON\n",
    "\n",
    "数据集地址：https://huggingface.co/datasets/merve/vqav2-small\n",
    "\n",
    "从本地 parquet 目录读取样本字节图像,导出到 `train_images/val_images`,并生成 `vqav2small_train.json` 与 `vqav2small_val.json`(messages/images 结构)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759ea8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取所有 parquet 文件...\n",
      "共加载 21435 条样本\n",
      "拆分结果：训练集 900 条，验证集 100 条\n",
      "开始导出 train 集\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:03<00:00, 299.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 集导出完成，共 900 条样本，图片保存在 /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/datasets/vqav2small/train_images\n",
      "开始导出 val 集\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 469.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 集导出完成，共 100 条样本，图片保存在 /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/datasets/vqav2small/val_images\n",
      "全部数据集导出完成。输出目录： /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/datasets/vqav2small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 配置：若你已将 parquet 下载到本地目录，修改此路径\n",
    "parquet_dir = os.path.join(datasets_dir.as_posix(), \"merve--vqav2-small\")\n",
    "output_base_dir = os.path.join(datasets_dir.as_posix(), \"vqav2small\")\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "max_samples = 1000  # 可改为 None 表示使用全部\n",
    "val_ratio = 0.1\n",
    "\n",
    "splits = {\n",
    "    \"train\": {\n",
    "        \"img_dir\": os.path.join(output_base_dir, \"train_images\"),\n",
    "        \"json_path\": os.path.join(output_base_dir, \"vqav2small_train.json\"),\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"img_dir\": os.path.join(output_base_dir, \"val_images\"),\n",
    "        \"json_path\": os.path.join(output_base_dir, \"vqav2small_val.json\"),\n",
    "    },\n",
    "}\n",
    "for split in splits.values():\n",
    "    os.makedirs(split[\"img_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"读取所有 parquet 文件...\")\n",
    "all_rows = []\n",
    "for file in sorted(os.listdir(parquet_dir)):\n",
    "    if not file.endswith(\".parquet\"):\n",
    "        continue\n",
    "    df = pd.read_parquet(os.path.join(parquet_dir, file))\n",
    "    for idx, row in df.iterrows():\n",
    "        all_rows.append((file, idx, row))\n",
    "print(f\"共加载 {len(all_rows)} 条样本\")\n",
    "\n",
    "random.seed(42)\n",
    "if max_samples and len(all_rows) > max_samples:\n",
    "    all_rows = random.sample(all_rows, max_samples)\n",
    "\n",
    "random.shuffle(all_rows)\n",
    "split_index = int(len(all_rows) * (1 - val_ratio))\n",
    "train_rows = all_rows[:split_index]\n",
    "val_rows = all_rows[split_index:]\n",
    "print(f\"拆分结果：训练集 {len(train_rows)} 条，验证集 {len(val_rows)} 条\")\n",
    "\n",
    "def export_split(split_name, rows):\n",
    "    cfg = splits[split_name]\n",
    "    json_output = []\n",
    "    print(f\"开始导出 {split_name} 集\")\n",
    "    for file, idx, row in tqdm(rows):\n",
    "        try:\n",
    "            img_bytes = row[\"image\"][\"bytes\"]\n",
    "            img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"图片解码失败: {file}_{idx}, 错误: {e}\")\n",
    "            continue\n",
    "        img_name = f\"{split_name}_{file}_{idx}.jpg\"\n",
    "        img_path = os.path.join(cfg[\"img_dir\"], img_name)\n",
    "        img.save(img_path)\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"<image>{row['question']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"multiple_choice_answer\"]},\n",
    "        ]\n",
    "        json_output.append({\"messages\": messages, \"images\": [img_path]})\n",
    "    with open(cfg[\"json_path\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_output, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{split_name} 集导出完成，共 {len(json_output)} 条样本，图片保存在 {cfg['img_dir']}\")\n",
    "\n",
    "export_split(\"train\", train_rows)\n",
    "export_split(\"val\", val_rows)\n",
    "print(\"全部数据集导出完成。输出目录：\", output_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23667ce",
   "metadata": {},
   "source": [
    "## 2.4 注册 Azure ML 数据资产"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa543272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/p_d2t4qx1pn8t41fwvpnykx00000gp/T/ipykernel_53321/3604369608.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  version=datetime.utcnow().strftime(\"%Y%m%d%H%M\"),\n",
      "\u001b[32mUploading vqav2small (50.14 MBs): 100%|██████████| 50137461/50137461 [00:14<00:00, 3489514.10it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data asset created: vqav2small-llamafactory:202510291437\n",
      "URI: azureml://subscriptions/7a03e9b8-18d6-48e7-b186-0ec68da9e86f/resourcegroups/aml-rg/workspaces/aml-hu-east-west-us2/datastores/workspaceblobstore/paths/LocalUpload/6f1951706f548c8a1401400c9ea42a54ff5a9e1f0262a418343b51ff5b29aeba/vqav2small/\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from datetime import datetime\n",
    "\n",
    "# 将 vqav2small 目录注册为 Data 资产\n",
    "vqav2_data_asset = Data(\n",
    "    name=\"vqav2small-llamafactory\",\n",
    "    version=datetime.utcnow().strftime(\"%Y%m%d%H%M\"),\n",
    "    path=output_base_dir,   # 目录中包含 train_images/val_images 以及 json\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"VQA vqav2-small preprocessed for LLaMA-Factory (messages/images)\"\n",
    ")\n",
    "registered_vqav2 = ml_client.data.create_or_update(vqav2_data_asset)\n",
    "print(f\"Data asset created: {registered_vqav2.name}:{registered_vqav2.version}\")\n",
    "print(\"URI:\", registered_vqav2.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d63d4e",
   "metadata": {},
   "source": [
    "### 注册后可在azure的资产页面中查看\n",
    "- ![查看资产](images/azure_ml_assets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7628bd",
   "metadata": {},
   "source": [
    "## 2.5 写入 LLaMA-Factory 训练配置 YAML(Qwen3-VL-4B LoRA)\n",
    "\n",
    "生成 `qwen3vl_lora_sft.yaml`,设置 Qwen3-VL-4B-Instruct、LoRA 超参、batch/epochs、日志与输出目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8f1e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入配置: /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/config/qwen3vl_lora_sft.yaml\n",
      "预期输出目录: /Users/chengbian/Documents/workspace/qwen_vl_lora/aml/workshop_qwen_vl/outputs/saves/qwen3vl-4b/lora/sft\n"
     ]
    }
   ],
   "source": [
    "yaml_path = (config_dir / \"qwen3vl_lora_sft.yaml\").as_posix()\n",
    "\n",
    "yaml_content = f\"\"\"\n",
    "### model\n",
    "model_name_or_path: Qwen/Qwen3-VL-4B-Instruct\n",
    "image_max_pixels: 262144\n",
    "video_max_pixels: 16384\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 16\n",
    "lora_target: all\n",
    "\n",
    "### dataset\n",
    "# 注意：此处使用自定义预处理后的 vqav2small_train.json；\n",
    "# LLaMA-Factory 读取 JSON 路径时请用绝对路径或相对当前工作目录\n",
    "# 如果需要，可改写为你的自定义数据集名称与路径\n",
    "# 这里沿用 workshop 命名，具体解析逻辑以 llamafactory 的数据适配为准\n",
    "\n",
    "# 兼容 workshop 的最小化配置；如需严格对齐 LF 官方数据注册，需在 data/dataset_info.json 中登记\n",
    "# 此处只作为演示（实际训练时可在命令中通过 --dataset xxx 或传 data_path 参数）\n",
    "dataset: vqav2small_train\n",
    "template: qwen3_vl\n",
    "cutoff_len: 2048\n",
    "max_samples: 1000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "dataloader_num_workers: 0\n",
    "\n",
    "### output\n",
    "output_dir: {outputs_dir.as_posix()}/saves/qwen3vl-4b/lora/sft\n",
    "logging_steps: 10\n",
    "save_steps: 200\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "save_only_model: false\n",
    "report_to: none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 2\n",
    "gradient_accumulation_steps: 8\n",
    "learning_rate: 1.0e-4\n",
    "num_train_epochs: 1.0\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "bf16: true\n",
    "ddp_timeout: 180000000\n",
    "resume_from_checkpoint: null\n",
    "\"\"\"\n",
    "\n",
    "with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"写入配置:\", yaml_path)\n",
    "print(\"预期输出目录:\", (outputs_dir / \"saves/qwen3vl-4b/lora/sft\").as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199acc6f",
   "metadata": {},
   "source": [
    "## 2.6 注册/创建 Azure ML 环境\n",
    "\n",
    "#### 🎯 镜像选择建议\n",
    "\n",
    "**当前配置（推荐）：HuggingFace NLP GPU 专用镜像**\n",
    "```python\n",
    "image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\"\n",
    "```\n",
    "✅ **优点：**\n",
    "- 专门为 HuggingFace Transformers 优化\n",
    "- 预装 CUDA 12.x，完美支持最新的 bitsandbytes\n",
    "- 针对 A100 GPU 优化\n",
    "- 包含 NLP 和多模态任务的常用库\n",
    "\n",
    "**其他可用选项：**\n",
    "\n",
    "| 镜像 | CUDA | 适用场景 | 推荐度 |\n",
    "|------|------|---------|--------|\n",
    "| `acft-hf-nlp-gpu:latest` | 12.x | HuggingFace + LoRA 微调 | ⭐⭐⭐⭐⭐ |\n",
    "| `pytorch-2.0-cuda11.7-cudnn8-ubuntu22.04:latest` | 11.7 | 通用 PyTorch 训练 | ⭐⭐⭐ |\n",
    "| `openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04:latest` | 11.8 | 分布式训练 | ⭐⭐⭐ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa44964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment registered: qwen3vl-lora-lf:2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "aml_env = Environment(\n",
    "    name=\"qwen3vl-lora-lf\",\n",
    "    description=\"Curated PyTorch image; pip installs LLaMA-Factory during job\",\n",
    "    image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
    ")\n",
    "registered_env = ml_client.environments.create_or_update(aml_env)\n",
    "print(f\"Environment registered: {registered_env.name}:{registered_env.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd0bed",
   "metadata": {},
   "source": [
    "### 完成操作后可以在环境页面进行查看\n",
    "\n",
    "- ![查看环境](images/azure_ml_environment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4305a4",
   "metadata": {},
   "source": [
    "## 2.7 创建并提交 LLaMA-Factory 训练命令作业(AML)\n",
    "\n",
    "### 计算资源准备\n",
    "1. 推荐使用带 A100 80GB 的 GPU（如 `Standard_NC24ads_A100_v4`）。\n",
    "2. 若尚未创建计算集群，可在 Azure ML Studio → 计算 → 计算集群 创建，或使用 Azure CLI：\n",
    "   ```bash\n",
    "   az ml compute create --name gpu-a100-cluster --type amlcompute \\\n",
    "       --resource-group <RG> --workspace-name <WS> \\\n",
    "       --min-instances 0 --max-instances 2 --size Standard_NC24ads_A100_v4\n",
    "   ```\n",
    "3. 训练前确认集群处于“空闲”或“已分配”状态，确保配额足够。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3637e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现有计算资源:\n",
      "  - A100-swedCentral (类型: virtualmachine, 大小: N/A, 状态: Succeeded)\n",
      "  - A100-centre-us (类型: virtualmachine, 大小: N/A, 状态: Succeeded)\n",
      "  - multi-model-batch-vm (类型: computeinstance, 大小: Standard_D96ads_v5, 状态: Succeeded)\n",
      "  - o1-performance-test-vm (类型: computeinstance, 大小: Standard_E4ds_v4, 状态: Succeeded)\n",
      "  - slm-fine-tune-lab (类型: computeinstance, 大小: Standard_E4ds_v4, 状态: Succeeded)\n",
      "  - gpu-cluster-a100 (类型: amlcompute, 大小: Standard_NC24ads_A100_v4, 状态: Succeeded)\n",
      "  - agent-demo-ws (类型: computeinstance, 大小: Standard_D32d_v4, 状态: Succeeded)\n",
      "  - notebook-llm-solu-vm (类型: computeinstance, 大小: Standard_D48a_v4, 状态: Succeeded)\n",
      "  - gpu-phi4-predict-out (类型: computeinstance, 大小: Standard_NC8as_T4_v3, 状态: Succeeded)\n",
      "  - Standard-NV12s-v3-m60 (类型: computeinstance, 大小: Standard_NV12s_v3, 状态: Succeeded)\n",
      "  - multi-gpus-trainer (类型: computeinstance, 大小: Standard_NC64as_T4_v3, 状态: Succeeded)\n",
      "  - a100-west-1 (类型: amlcompute, 大小: Standard_NC24ads_A100_v4, 状态: Succeeded)\n",
      "  - qwen-fine-tune-A100 (类型: computeinstance, 大小: Standard_NC48ads_A100_v4, 状态: Succeeded)\n",
      "  - qwen-fine-tune-H100 (类型: amlcompute, 大小: Standard_NC80adis_H100_v5, 状态: Succeeded)\n",
      "  - qwen-25-vl-finetune (类型: computeinstance, 大小: Standard_NC80adis_H100_v5, 状态: Succeeded)\n",
      "\n",
      "计算集群 'qwen-fine-tune-H100' 已存在，状态: Succeeded\n"
     ]
    }
   ],
   "source": [
    "# 检查现有计算资源并在需要时创建 GPU 集群\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# 列出所有现有的计算资源\n",
    "print(\"现有计算资源:\")\n",
    "for compute in ml_client.compute.list():\n",
    "    print(f\"  - {compute.name} (类型: {compute.type}, 大小: {getattr(compute, 'size', 'N/A')}, 状态: {compute.provisioning_state})\")\n",
    "\n",
    "# 定义计算集群名称\n",
    "COMPUTE_NAME = \"qwen-fine-tune-H100\"\n",
    "\n",
    "# 检查计算集群是否存在，如果不存在则创建\n",
    "try:\n",
    "    compute_target = ml_client.compute.get(COMPUTE_NAME)\n",
    "    print(f\"\\n计算集群 '{COMPUTE_NAME}' 已存在，状态: {compute_target.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n计算集群 '{COMPUTE_NAME}' 不存在，正在创建...\")\n",
    "    \n",
    "    # 创建计算集群配置\n",
    "    # 注意：Standard_NC24ads_A100_v4 在某些区域可能不可用\n",
    "    # 可替换为其他 GPU SKU，如 Standard_NC6s_v3, Standard_NC12s_v3 等\n",
    "    compute_config = AmlCompute(\n",
    "        name=COMPUTE_NAME,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_NC80adis_H100_v5\",  # A100 80GB GPU\n",
    "        min_instances=0,\n",
    "        max_instances=1,\n",
    "        idle_time_before_scale_down=300,  # 5分钟后自动缩减\n",
    "        tier=\"dedicated\",\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        compute_target = ml_client.compute.begin_create_or_update(compute_config).result()\n",
    "        print(f\"计算集群 '{COMPUTE_NAME}' 创建成功！\")\n",
    "    except Exception as create_error:\n",
    "        print(f\"创建失败: {create_error}\")\n",
    "        print(\"\\n可能的原因:\")\n",
    "        print(\"1. 所选 GPU SKU 在当前区域不可用\")\n",
    "        print(\"2. GPU 配额不足\")\n",
    "        print(\"\\n建议:\")\n",
    "        print(\"- 在 Azure Portal 中检查可用的 VM SKU\")\n",
    "        print(\"- 请求增加 GPU 配额\")\n",
    "        print(\"- 或使用现有的计算资源（如果上面列出了可用的计算）\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef4614",
   "metadata": {},
   "source": [
    "### 完成计算资源准备后， 提交训练命令\n",
    "命令会在计算节点上:\n",
    "1) 克隆 LLaMA-Factory 并安装;2) 设置 HF 镜像与加速;3) 读取挂载的数据目录;4) 调用 `llamafactory-cli train` 使用上文 YAML 配置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2fe2bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'qwen3vl-lora-lf' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'qwen3vl-lora-lf' will not be used for anonymous registration\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted: cool_head_rrkrh8znqf\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import Input, Output, command\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# 计算与实验配置（请根据你的工作区实际值修改）\n",
    "EXPERIMENT_NAME = \"qwen3vl-lora-lf\"\n",
    "\n",
    "# 使用上面注册的 Data 资产作为输入\n",
    "data_input = Input(type=AssetTypes.URI_FOLDER, path=registered_vqav2.id)\n",
    "\n",
    "# 训练命令：在节点上安装 LLaMA-Factory 并运行训练\n",
    "# 注意：${{inputs.data}} 为 Azure ML 在运行时注入的挂载路径\n",
    "train_cmd = \" && \".join([\n",
    "    \"set -e\",\n",
    "    \"echo \\\"Python:\\\" $(python --version)\",\n",
    "    \"echo \\\"Pip:\\\" $(pip --version)\",\n",
    "    \"git clone https://github.com/hiyouga/LLaMA-Factory.git\",\n",
    "    \"cd LLaMA-Factory\",\n",
    "    \"export HF_HUB_ENABLE_HF_TRANSFER=1\",\n",
    "    \"if [ \\\"$AZURE_CLOUD_NAME\\\" = \\\"AzureChinaCloud\\\" ]; then export HF_ENDPOINT=https://hf-mirror.com; fi\",\n",
    "    \"pip install -e \\\".[torch,metrics]\\\" --no-build-isolation\",\n",
    "    f\"llamafactory-cli train {yaml_path}\",\n",
    "])\n",
    "\n",
    "job = command(\n",
    "    code=str(workspace_dir),\n",
    "    command=train_cmd,\n",
    "    inputs={\"data\": data_input},\n",
    "    environment=registered_env,\n",
    "    compute=COMPUTE_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    display_name=\"qwen3vl-lora-train-lf\",\n",
    "    outputs={\n",
    "        \"trained\": Output(type=AssetTypes.URI_FOLDER)  # 将默认写入作业输出目录\n",
    "    },\n",
    "    description=\"Qwen3-VL-4B LoRA training via LLaMA-Factory\"\n",
    ")\n",
    "\n",
    "# 以挂载方式访问数据（提升 I/O 速度）\n",
    "job.inputs[\"data\"].mode = \"mount\"\n",
    "submitted_job = ml_client.jobs.create_or_update(job)\n",
    "print(\"Job submitted:\", submitted_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fa95c",
   "metadata": {},
   "source": [
    "## 2.8 监控训练日志(AML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(submitted_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401c00c",
   "metadata": {},
   "source": [
    "## 2.9 下载训练输出并展示损失曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, tempfile\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# 下载作业产物到本地临时目录\n",
    "local_download = pathlib.Path(tempfile.mkdtemp(prefix=\"aml-train-\"))\n",
    "ml_client.jobs.download(submitted_job.name, download_path=local_download.as_posix())\n",
    "print(\"Downloaded to:\", local_download)\n",
    "\n",
    "# 搜索 training_loss.png\n",
    "loss_png = None\n",
    "for p in local_download.rglob(\"training_loss.png\"):\n",
    "    loss_png = p\n",
    "    break\n",
    "\n",
    "if loss_png and loss_png.exists():\n",
    "    print(\"Found:\", loss_png)\n",
    "    display(Image.open(loss_png).convert(\"RGB\"))\n",
    "else:\n",
    "    print(\"未找到 training_loss.png。请检查作业输出目录结构。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738b5aa",
   "metadata": {},
   "source": [
    "## 2.10 合并 LoRA 适配器为完整模型(AML 导出作业)\n",
    "\n",
    "使用第二个命令作业在计算节点上运行 `llamafactory-cli export`,将前一作业的 LoRA 适配器与基础模型合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "\n",
    "# 将上一个作业的默认 outputs 作为输入传入\n",
    "prev_outputs_uri = f\"azureml://jobs/{submitted_job.name}/outputs\"\n",
    "prev_input = Input(type=AssetTypes.URI_FOLDER, path=prev_outputs_uri)\n",
    "\n",
    "merge_cmd = \" && \".join([\n",
    "    \"set -e\",\n",
    "    \"git clone https://github.com/hiyouga/LLaMA-Factory.git\",\n",
    "    \"cd LLaMA-Factory\",\n",
    "    \"pip install -e \\\".[torch,metrics]\\\" --no-build-isolation\",\n",
    "    # 选择最近的 checkpoint 目录\n",
    "    \"ADAPTER=$(ls -d ${inputs.prev}/saves/qwen3vl-4b/lora/sft/checkpoint-* | sort | tail -n 1)\",\n",
    "    \"echo Using adapter: $ADAPTER\",\n",
    "    # 生成合并配置文件\n",
    "    \"cat > merge_lora.yaml <<'EOF'\\n\"\n",
    "    \"### model\\n\"\n",
    "    \"model_name_or_path: Qwen/Qwen3-VL-4B-Instruct\\n\"\n",
    "    \"adapter_name_or_path: $ADAPTER\\n\"\n",
    "    \"template: qwen3_vl\\n\"\n",
    "    \"trust_remote_code: true\\n\\n\"\n",
    "    \"### export\\n\"\n",
    "    \"export_dir: ./outputs/merged\\n\"\n",
    "    \"export_size: 5\\n\"\n",
    "    \"export_device: cpu\\n\"\n",
    "    \"export_legacy_format: false\\n\"\n",
    "    \"EOF\",\n",
    "    # 执行导出\n",
    "    \"llamafactory-cli export merge_lora.yaml\",\n",
    "])\n",
    "\n",
    "merge_job = command(\n",
    "    code=str(workspace_dir),\n",
    "    command=merge_cmd,\n",
    "    inputs={\"prev\": prev_input},\n",
    "    environment=registered_env,\n",
    "    compute=COMPUTE_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    display_name=\"qwen3vl-merge-lora\",\n",
    "    outputs={\"merged\": Output(type=AssetTypes.URI_FOLDER)},\n",
    "    description=\"Merge LoRA adapter into full model\"\n",
    ")\n",
    "\n",
    "merge_job.inputs[\"prev\"].mode = \"download\"  # 读取上次作业产物\n",
    "submitted_merge = ml_client.jobs.create_or_update(merge_job)\n",
    "print(\"Merge job submitted:\", submitted_merge.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc694df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监控合并作业\n",
    "ml_client.jobs.stream(submitted_merge.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9289b",
   "metadata": {},
   "source": [
    "## 2.11 注册 LoRA 适配器/合并模型为 Azure ML 模型资产"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "# 适配器模型（注册训练作业的 outputs 目录）\n",
    "adapter_uri = f\"azureml://jobs/{submitted_job.name}/outputs\"\n",
    "adapter_model = Model(\n",
    "    name=\"qwen3vl-lora-adapter-lf\",\n",
    "    path=adapter_uri,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"LoRA adapter trained via LLaMA-Factory on AML\",\n",
    "    tags={\"base_model\": BASE_MODEL, \"task\": \"sft\", \"tool\": \"llamafactory\"}\n",
    ")\n",
    "reg_adapter = ml_client.models.create_or_update(adapter_model)\n",
    "print(\"Adapter model:\", f\"{reg_adapter.name}:{reg_adapter.version}\")\n",
    "\n",
    "# 合并后完整模型（注册导出作业的 merged 输出）\n",
    "merged_uri = f\"azureml://jobs/{submitted_merge.name}/outputs/merged\"\n",
    "full_model = Model(\n",
    "    name=\"qwen3vl-merged-lf\",\n",
    "    path=merged_uri,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Full model merged from base + LoRA via LLaMA-Factory export\",\n",
    "    tags={\"base_model\": BASE_MODEL, \"merged\": \"true\", \"tool\": \"llamafactory\"}\n",
    ")\n",
    "reg_full = ml_client.models.create_or_update(full_model)\n",
    "print(\"Merged model:\", f\"{reg_full.name}:{reg_full.version}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
