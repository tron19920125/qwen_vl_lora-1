{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8485df37",
   "metadata": {},
   "source": [
    "# 在 Azure Machine Learning 上使用 LoRA 微调 Qwen-VL\n",
    "\n",
    "本笔记演示如何在 Azure Machine Learning (Azure ML) 上对 Qwen-VL 系列多模态大模型执行 LoRA 指令微调，并在训练完成后注册模型及验证推理结果。流程涵盖环境配置、数据准备、训练脚本编写、作业提交与监控等关键步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373c68b",
   "metadata": {},
   "source": [
    "## 前提条件\n",
    "\n",
    "- Azure 订阅以及已创建的 Azure ML 工作区。\n",
    "- 工作区所在区域已经申请到足够的 GPU 配额（推荐 `Standard_NC24ads_A100_v4`）。\n",
    "- 本地或计算实例已安装 `azure-ai-ml>=1.15.0` 与 `azure-identity>=1.16.0`。\n",
    "- 若使用托管身份或服务主体，请确保具备对工作区的访问权限。\n",
    "- 准备图文对训练数据，推荐整理为 JSON Lines 格式（示例见后文）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d582b",
   "metadata": {},
   "source": [
    "## 操作文档概览\n",
    "\n",
    "执行本指南时建议按以下顺序完成：\n",
    "1. 在本地整理数据（图片 + 标注 JSONL），并根据示例检查字段。\n",
    "2. 将数据上传/注册到 Azure ML 数据存储或数据资产。\n",
    "3. 检查或创建 GPU 计算集群。\n",
    "4. 运行 Notebook 前半部分生成并注册训练环境、上传脚本。\n",
    "5. 根据实际资源修改命令作业参数并提交训练。\n",
    "6. 监控日志、下载输出，最后注册模型并本地验证。\n",
    "7. （可选）基于注册模型继续部署或批量推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ab1b6",
   "metadata": {},
   "source": [
    "## 连接到 Azure ML 工作区\n",
    "\n",
    "填写订阅 ID、资源组与工作区名称。以下示例优先使用 `DefaultAzureCredential`，若本地无可用身份则回退到交互式浏览器登录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6815208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to workspace: aml-hu-east-west-us2\n",
      "Location: westus2\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "import os\n",
    "\n",
    "# 读取工作区基本信息，优先从环境变量中获取\n",
    "SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\", \"7a03e9b8-18d6-48e7-b186-0ec68da9e86f\")\n",
    "RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\", \"aml-rg\")\n",
    "WORKSPACE_NAME = os.getenv(\"AZUREML_WORKSPACE_NAME\", \"aml-hu-east-west-us2\")\n",
    "\n",
    "# 交互式登录\n",
    "# credential = InteractiveBrowserCredential(tenant_id=\"16b3c013-d300-468d-ac64-7eda0820b6d3\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# 初始化 MLClient 以便后续操作工作区资源\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    " )\n",
    "\n",
    "print(f\"Connected to workspace: {ml_client.workspace_name}\")\n",
    "workspace = ml_client.workspaces.get(ml_client.workspace_name)  # 读取工作区以获取区域信息\n",
    "print(f\"Location: {workspace.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaee20e",
   "metadata": {},
   "source": [
    "## 准备目录结构与环境文件\n",
    "\n",
    "在工作目录下创建源码与环境配置文件夹，并写入将用于训练作业的 Conda 依赖。环境基于 Azure ML 官方 PyTorch CUDA 12.1 镜像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8903c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source directory: /home/azureuser/qwen_vl_lora/qwen_vl_lora/src\n",
      "Environment directory: /home/azureuser/qwen_vl_lora/qwen_vl_lora/env\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# 在本地构建源码和环境目录，用于组织训练脚本与依赖文件\n",
    "workspace_dir = pathlib.Path.cwd() / \"qwen_vl_lora\"\n",
    "src_dir = workspace_dir / \"src\"\n",
    "env_dir = workspace_dir / \"env\"\n",
    "workspace_dir.mkdir(exist_ok=True)\n",
    "src_dir.mkdir(parents=True, exist_ok=True)\n",
    "env_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Source directory: {src_dir}\")\n",
    "print(f\"Environment directory: {env_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b0a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/azureuser/qwen_vl_lora/qwen_vl_lora/env/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {env_dir}/conda.yaml\n",
    "# Conda 环境描述，Azure ML 会据此在基础镜像上安装额外依赖\n",
    "name: qwen-vl-lora-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - accelerate>=0.28.0\n",
    "    - bitsandbytes>=0.43.0\n",
    "    - datasets>=2.18.0\n",
    "    - peft>=0.11.0\n",
    "    - pillow>=10.2.0\n",
    "    - sentencepiece>=0.2.0\n",
    "    - timm>=0.9.12\n",
    "    - torch>=2.3.0\n",
    "    - torchvision>=0.18.0\n",
    "    - transformers>=4.39.0\n",
    "    - trl>=0.8.6\n",
    "    - wandb>=0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "463bb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment registered: qwen-vl-lora-env-demo-03:1\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "# 将自定义 Conda 依赖与官方基础镜像组合成 Azure ML 环境\n",
    "# 使用 HuggingFace NLP 专用镜像（推荐用于 Transformers + LoRA 微调）\n",
    "qwen_env = Environment(\n",
    "    name=\"qwen-vl-lora-env-demo-03\",\n",
    "    description=\"LoRA finetuning environment for Qwen-VL demo 03\",\n",
    "    conda_file=str((env_dir / \"conda.yaml\").as_posix()),\n",
    "    # 方案 1：HuggingFace NLP GPU 专用镜像（推荐）\n",
    "    # 已预装 transformers, CUDA 12.x, 针对 A100 优化\n",
    "    image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
    ")\n",
    "registered_env = ml_client.environments.create_or_update(qwen_env)\n",
    "print(f\"Environment registered: {registered_env.name}:{registered_env.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd411fe",
   "metadata": {},
   "source": [
    "### 注册训练环境\n",
    "\n",
    "创建并注册包含所需依赖的 Azure ML 环境。根据您的需求选择合适的基础镜像。\n",
    "\n",
    "#### 🎯 镜像选择建议\n",
    "\n",
    "**当前配置（推荐）：HuggingFace NLP GPU 专用镜像**\n",
    "```python\n",
    "image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\"\n",
    "```\n",
    "✅ **优点：**\n",
    "- 专门为 HuggingFace Transformers 优化\n",
    "- 预装 CUDA 12.x，完美支持最新的 bitsandbytes\n",
    "- 针对 A100 GPU 优化\n",
    "- 包含 NLP 和多模态任务的常用库\n",
    "\n",
    "**其他可用选项：**\n",
    "\n",
    "| 镜像 | CUDA | 适用场景 | 推荐度 |\n",
    "|------|------|---------|--------|\n",
    "| `acft-hf-nlp-gpu:latest` | 12.x | HuggingFace + LoRA 微调 | ⭐⭐⭐⭐⭐ |\n",
    "| `pytorch-2.0-cuda11.7-cudnn8-ubuntu22.04:latest` | 11.7 | 通用 PyTorch 训练 | ⭐⭐⭐ |\n",
    "| `openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04:latest` | 11.8 | 分布式训练 | ⭐⭐⭐ |\n",
    "\n",
    "#### ⚠️ 重要提示\n",
    "- **bitsandbytes 0.43.0+** 需要 CUDA 12.x 以获得最佳性能\n",
    "- **Qwen-VL 多模态模型** 在 HuggingFace 镜像上运行更稳定\n",
    "- **A100 GPU** 在 CUDA 12.x 上性能更优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314ef2f",
   "metadata": {},
   "source": [
    "### 计算资源准备\n",
    "1. 推荐使用带 A100 80GB 的 GPU（如 `Standard_NC24ads_A100_v4`）。\n",
    "2. 若尚未创建计算集群，可在 Azure ML Studio → 计算 → 计算集群 创建，或使用 Azure CLI：\n",
    "   ```bash\n",
    "   az ml compute create --name gpu-a100-cluster --type amlcompute \\\n",
    "       --resource-group <RG> --workspace-name <WS> \\\n",
    "       --min-instances 0 --max-instances 2 --size Standard_NC24ads_A100_v4\n",
    "   ```\n",
    "3. 训练前确认集群处于“空闲”或“已分配”状态，确保配额足够。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91fcd393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现有计算资源:\n",
      "  - A100-swedCentral (类型: virtualmachine, 大小: N/A, 状态: Succeeded)\n",
      "  - A100-centre-us (类型: virtualmachine, 大小: N/A, 状态: Succeeded)\n",
      "  - multi-model-batch-vm (类型: computeinstance, 大小: Standard_D96ads_v5, 状态: Succeeded)\n",
      "  - o1-performance-test-vm (类型: computeinstance, 大小: Standard_E4ds_v4, 状态: Succeeded)\n",
      "  - slm-fine-tune-lab (类型: computeinstance, 大小: Standard_E4ds_v4, 状态: Succeeded)\n",
      "  - gpu-cluster-a100 (类型: amlcompute, 大小: Standard_NC24ads_A100_v4, 状态: Succeeded)\n",
      "  - agent-demo-ws (类型: computeinstance, 大小: Standard_D32d_v4, 状态: Succeeded)\n",
      "  - notebook-llm-solu-vm (类型: computeinstance, 大小: Standard_D48a_v4, 状态: Succeeded)\n",
      "  - gpu-phi4-predict-out (类型: computeinstance, 大小: Standard_NC8as_T4_v3, 状态: Succeeded)\n",
      "  - Standard-NV12s-v3-m60 (类型: computeinstance, 大小: Standard_NV12s_v3, 状态: Succeeded)\n",
      "  - multi-gpus-trainer (类型: computeinstance, 大小: Standard_NC64as_T4_v3, 状态: Succeeded)\n",
      "  - a100-west-1 (类型: amlcompute, 大小: Standard_NC24ads_A100_v4, 状态: Succeeded)\n",
      "  - qwen-fine-tune-A100 (类型: computeinstance, 大小: Standard_NC48ads_A100_v4, 状态: Succeeded)\n",
      "  - qwen-fine-tune-H100 (类型: amlcompute, 大小: Standard_NC80adis_H100_v5, 状态: Succeeded)\n",
      "\n",
      "计算集群 'qwen-fine-tune-H100' 已存在，状态: Succeeded\n"
     ]
    }
   ],
   "source": [
    "# 检查现有计算资源并在需要时创建 GPU 集群\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# 列出所有现有的计算资源\n",
    "print(\"现有计算资源:\")\n",
    "for compute in ml_client.compute.list():\n",
    "    print(f\"  - {compute.name} (类型: {compute.type}, 大小: {getattr(compute, 'size', 'N/A')}, 状态: {compute.provisioning_state})\")\n",
    "\n",
    "# 定义计算集群名称\n",
    "COMPUTE_NAME = \"qwen-fine-tune-H100\"\n",
    "\n",
    "# 检查计算集群是否存在，如果不存在则创建\n",
    "try:\n",
    "    compute_target = ml_client.compute.get(COMPUTE_NAME)\n",
    "    print(f\"\\n计算集群 '{COMPUTE_NAME}' 已存在，状态: {compute_target.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n计算集群 '{COMPUTE_NAME}' 不存在，正在创建...\")\n",
    "    \n",
    "    # 创建计算集群配置\n",
    "    # 注意：Standard_NC24ads_A100_v4 在某些区域可能不可用\n",
    "    # 可替换为其他 GPU SKU，如 Standard_NC6s_v3, Standard_NC12s_v3 等\n",
    "    compute_config = AmlCompute(\n",
    "        name=COMPUTE_NAME,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_NC80adis_H100_v5\",  # A100 80GB GPU\n",
    "        min_instances=0,\n",
    "        max_instances=1,\n",
    "        idle_time_before_scale_down=300,  # 5分钟后自动缩减\n",
    "        tier=\"dedicated\",\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        compute_target = ml_client.compute.begin_create_or_update(compute_config).result()\n",
    "        print(f\"计算集群 '{COMPUTE_NAME}' 创建成功！\")\n",
    "    except Exception as create_error:\n",
    "        print(f\"创建失败: {create_error}\")\n",
    "        print(\"\\n可能的原因:\")\n",
    "        print(\"1. 所选 GPU SKU 在当前区域不可用\")\n",
    "        print(\"2. GPU 配额不足\")\n",
    "        print(\"\\n建议:\")\n",
    "        print(\"- 在 Azure Portal 中检查可用的 VM SKU\")\n",
    "        print(\"- 请求增加 GPU 配额\")\n",
    "        print(\"- 或使用现有的计算资源（如果上面列出了可用的计算）\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699f1d9",
   "metadata": {},
   "source": [
    "### 检查并创建计算集群\n",
    "\n",
    "运行下方代码单元格将：\n",
    "1. 列出工作区中所有现有的计算资源\n",
    "2. 检查 `gpu-a100-cluster` 是否存在\n",
    "3. 如果不存在，自动创建该集群\n",
    "\n",
    "**注意事项：**\n",
    "- 如果创建失败（配额不足或 SKU 不可用），可以：\n",
    "  - 在 Azure Portal 申请 GPU 配额\n",
    "  - 修改代码使用其他可用的 GPU SKU（如 `Standard_NC6s_v3`）\n",
    "  - 或使用上面列出的现有计算资源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c355894",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "\n",
    "> 目标：整理图文多轮对话数据，并转换为 LoRA 训练所需的 JSON Lines 文件。\n",
    "\n",
    "### 1. 本地目录结构建议\n",
    "- `dataset/`\b根目录\n",
    "  - `images/`\b存放所有训练图片，可按需要再分子目录。\n",
    "  - `train.jsonl`\b训练集标注文件。\n",
    "  - `validation.jsonl`\b验证集（可选）。\n",
    "\n",
    "### 2. JSONL 单条样本示例\n",
    "```json\n",
    "{\n",
    "  \"image\": \"images/sample_0001.jpg\",\n",
    "  \"question\": \"描述图片中的主要场景。\",\n",
    "  \"answer\": \"图片展示了一位骑行者在海边公路上骑行，阳光明媚。\",\n",
    "  \"system\": \"你是一名图文理解助手。\"\n",
    "}\n",
    "```\n",
    "- `image`\b可以是相对路径（相对于数据集根目录）或绝对路径；\n",
    "- `question`\b为用户指令/提问；\n",
    "- `answer`\b为模型期望回应；\n",
    "- `system`\b可选，用于提供系统提示或角色设定；\n",
    "- 可扩展额外字段（如标签、难度）但需在训练脚本中自行处理。\n",
    "\n",
    "### 3. 将其他标注格式转为 JSONL\n",
    "若已有 CSV/Excel/JSON，可在本地运行以下示例脚本生成 `train.jsonl`：\n",
    "```python\n",
    "import csv, json, pathlib\n",
    "\n",
    "input_csv = pathlib.Path(\"./raw.csv\")\n",
    "output_jsonl = pathlib.Path(\"./dataset/train.jsonl\")\n",
    "\n",
    "with input_csv.open(\"r\", encoding=\"utf-8\") as fin, output_jsonl.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    reader = csv.DictReader(fin)\n",
    "    for row in reader:\n",
    "        record = {\n",
    "            \"image\": f\"images/{row['image_filename']}\",\n",
    "            \"question\": row[\"instruction\"],\n",
    "            \"answer\": row[\"response\"],\n",
    "            \"system\": row.get(\"system_prompt\") or \"你是一名视觉助手。\"\n",
    "        }\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "```\n",
    "脚本执行后确认：\n",
    "1. JSONL 每行均为合法 JSON；\n",
    "2. 引用的图片文件均存在且可打开；\n",
    "3. 验证集（若存在）与训练集结构一致即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec4705",
   "metadata": {},
   "source": [
    "转换完成后重复前述上传步骤，将新生成的 JSONL 文件和图片目录同步到 Azure ML 数据存储即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf1eca",
   "metadata": {},
   "source": [
    "### 5. 从 Parquet/Arrow 数据源生成 JSONL\n",
    "若你的训练数据保存在 `test-00000-of-00001.parquet` 等 Parquet 文件中，可按以下步骤转换：\n",
    "1. 使用 `datasets` 或 `pyarrow` 读取 Parquet；\n",
    "2. 逐行构建所需字段并写入 JSONL；\n",
    "3. 同时批量导出引用到的图片（若存储为二进制或远程 URL）。\n",
    "\n",
    "下面示例 Parquet 中包含字段 'id', 'category', 'images', 'question', 'question_text', 'answer', 'difficulty', 'metric_info', 'initial_state'，并且 `images` 已指向本地图片："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e061c",
   "metadata": {},
   "source": [
    "如 Parquet 中包含嵌入二进制图像，可先使用 `dataset[i][\"image\"].save(...)` 将其导出到 `images/` 目录，再在生成 JSONL 时写入相对路径。若 `image_path` 为远程 URL，需提前下载到本地后再写入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02885af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json, pathlib\n",
    "from PIL import Image\n",
    "\n",
    "parquet_path = \"./dataset/test-00000-of-00001.parquet\"  # 修改为你的 parquet 路径\n",
    "output_jsonl = pathlib.Path(\"./dataset/train.jsonl\")\n",
    "images_dir = pathlib.Path(\"./dataset/images\")\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=parquet_path, split=\"train\")\n",
    "\n",
    "def save_image_as_png(image: Image.Image, image_path: pathlib.Path) -> str:\n",
    "    \"\"\"统一将图片保存为 PNG，保持 Alpha 通道并避免压缩损失。\"\"\"\n",
    "    if image.mode in (\"RGBA\", \"LA\", \"P\"):\n",
    "        image = image.convert(\"RGBA\")\n",
    "    else:\n",
    "        image = image.convert(\"RGB\")\n",
    "    image.save(image_path, format=\"PNG\")\n",
    "    return f\"images/{image_path.name}\"\n",
    "\n",
    "def resolve_image_field(example, sample_idx: int) -> str:\n",
    "    \"\"\"将 parquet 中的 images 字段转换为 JSONL 所需的图片路径。\"\"\"\n",
    "    image_field = example.get(\"images\")\n",
    "    if isinstance(image_field, list):\n",
    "        image_field = image_field[0] if image_field else None\n",
    "    if isinstance(image_field, Image.Image):\n",
    "        image_path = images_dir / f\"{sample_idx:06d}.png\"\n",
    "        return save_image_as_png(image_field, image_path)\n",
    "    if hasattr(image_field, \"save\"):\n",
    "        pil_image = image_field\n",
    "        image_path = images_dir / f\"{sample_idx:06d}.png\"\n",
    "        return save_image_as_png(pil_image, image_path)\n",
    "    if isinstance(image_field, dict):\n",
    "        candidate = image_field.get(\"path\") or image_field.get(\"image_path\") or image_field.get(\"url\")\n",
    "        if candidate:\n",
    "            return candidate\n",
    "    if isinstance(image_field, str):\n",
    "        return image_field\n",
    "    raise ValueError(\"无法解析 parquet 中的 images 字段，请检查数据格式\")\n",
    "\n",
    "with output_jsonl.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, example in enumerate(dataset):\n",
    "        record = {\n",
    "            \"image\": resolve_image_field(example, idx),\n",
    "            \"question\": example.get(\"question_text\") or example.get(\"question\") or \"\",\n",
    "            \"answer\": example.get(\"answer\", \"\"),\n",
    "            \"system\": example.get(\"initial_state\") or \"你是一名视觉助手。\",\n",
    "            \"category\": example.get(\"category\"),\n",
    "            \"metadata\": {\"difficulty\": example.get(\"difficulty\"), \"metric_info\": example.get(\"metric_info\")},\n",
    "        }\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Wrote {len(dataset)} samples to {output_jsonl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f398f5b",
   "metadata": {},
   "source": [
    "### 4. 将本地数据上传到 Azure ML\n",
    "1. **确认默认数据存储**：运行下方代码查看 `workspaceblobstore` 或自定义数据存储的名称。\n",
    "2. **上传/注册数据资产**：可使用 Notebook 中的 Python 代码，也可通过 Azure ML Studio 的“数据”页面手动上传。\n",
    "3. **保持目录结构**：上传时确保 `images/` 等子目录与 JSONL 文件保持原有层级。\n",
    "4. **数据更新**：若重新上传，可选择新版本号，避免覆盖历史数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "627a1a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml_globaldatasets -> DatastoreType.AZURE_BLOB\n",
      "workspaceblobstore -> DatastoreType.AZURE_BLOB\n",
      "workspaceartifactstore -> DatastoreType.AZURE_BLOB\n",
      "workspaceworkingdirectory -> DatastoreType.AZURE_FILE\n",
      "workspacefilestore -> DatastoreType.AZURE_FILE\n"
     ]
    }
   ],
   "source": [
    "# 查看当前工作区内可用的数据存储（默认通常为 workspaceblobstore）\n",
    "for ds in ml_client.datastores.list():\n",
    "    print(ds.name, \"->\", ds.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ff4f0",
   "metadata": {},
   "source": [
    "> **提示**：如果数据量较大，可使用 `azcopy` 或 Azure Storage Explorer 先上传到 Blob，再注册为数据资产。下方示例适合直接从本地文件夹上传。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fabb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'timezone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_dataset_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m本地数据目录不存在: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_dataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m data_asset \u001b[38;5;241m=\u001b[39m Data(\n\u001b[1;32m     13\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen-vl-instruction-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 14\u001b[0m     version\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimezone\u001b[49m\u001b[38;5;241m.\u001b[39mutc)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     path\u001b[38;5;241m=\u001b[39mlocal_dataset_path\u001b[38;5;241m.\u001b[39mas_posix(),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mAssetTypes\u001b[38;5;241m.\u001b[39mURI_FOLDER,\n\u001b[1;32m     17\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen-VL LoRA instruction dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m registered_data \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcreate_or_update(data_asset)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData asset created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistered_data\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistered_data\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'timezone'"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "# 将本地文件夹上传并注册为数据资产\n",
    "local_dataset_path = pathlib.Path(\"./dataset\")  # 修改为你的本地数据集路径\n",
    "if not local_dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"本地数据目录不存在: {local_dataset_path}\")\n",
    "\n",
    "data_asset = Data(\n",
    "    name=\"qwen-vl-instruction-data\",\n",
    "    version=datetime.utcnow().strftime(\"%Y%m%d%H%M\"),\n",
    "    path=local_dataset_path.as_posix(),\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Qwen-VL LoRA instruction dataset\",\n",
    ")\n",
    "\n",
    "registered_data = ml_client.data.create_or_update(data_asset)\n",
    "print(f\"Data asset created: {registered_data.name}:{registered_data.version}\")\n",
    "print(f\"Asset path: {registered_data.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118ff8e",
   "metadata": {},
   "source": [
    "运行成功后，记下 `Data asset created` 输出的 ID/路径，并将其填写到后续 `DATASET_PATH` 变量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {src_dir}/train_lora.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    )\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    \"\"\"解析命令行参数，便于在 Azure ML 作业中灵活传参。\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"LoRA finetuning for Qwen-VL demo 03\")\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "    parser.add_argument(\"--dataset-dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train.jsonl\")\n",
    "    parser.add_argument(\"--validation-file\", type=str, default=None)\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"./outputs\")\n",
    "    parser.add_argument(\"--per-device-train-batch-size\", type=int, default=1)\n",
    "    parser.add_argument(\"--per-device-eval-batch-size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=8)\n",
    "    parser.add_argument(\"--num-train-epochs\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--warmup-ratio\", type=float, default=0.03)\n",
    "    parser.add_argument(\"--logging-steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--save-strategy\", type=str, default=\"epoch\")\n",
    "    parser.add_argument(\"--eval-strategy\", type=str, default=\"epoch\")\n",
    "    parser.add_argument(\"--lora-rank\", type=int, default=64)\n",
    "    parser.add_argument(\"--lora-alpha\", type=int, default=128)\n",
    "    parser.add_argument(\"--lora-dropout\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--target-modules\", type=str, default=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\")\n",
    "    parser.add_argument(\"--bf16\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--trust-remote-code\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--report-to\", type=str, default=\"none\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def load_records(dataset_dir: str, file_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"读取 JSONL 文件，并返回样本列表。\"\"\"\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    logger.info(\"Loaded %d samples from %s\", len(records), file_path)\n",
    "    return records\n",
    "\n",
    "def resolve_image_path(dataset_dir: str, image_path: str) -> str:\n",
    "    \"\"\"统一处理相对路径，便于在 Azure ML 计算节点上访问图片。\"\"\"\n",
    "    return image_path if os.path.isabs(image_path) else os.path.join(dataset_dir, image_path)\n",
    "\n",
    "@dataclass\n",
    "class QwenRecord:\n",
    "    \"\"\"用 dataclass 存储单条样本，提升可读性。\"\"\"\n",
    "    image: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    system: Optional[str] = None\n",
    "\n",
    "class QwenVLDataset(Dataset):\n",
    "    \"\"\"将原始 JSON 样本转换为模型可直接使用的提示格式。\"\"\"\n",
    "    def __init__(self, records: List[Dict[str, Any]], dataset_dir: str, processor: AutoProcessor):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.processor = processor\n",
    "        self.records: List[QwenRecord] = [\n",
    "            QwenRecord(\n",
    "                image=resolve_image_path(dataset_dir, item[\"image\"]),\n",
    "                question=item.get(\"question\", \"\"),\n",
    "                answer=item.get(\"answer\", \"\"),\n",
    "                system=item.get(\"system\")\n",
    "            )\n",
    "            for item in records\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        record = self.records[idx]\n",
    "        if not os.path.exists(record.image):\n",
    "            raise FileNotFoundError(f\"Image not found: {record.image}\")\n",
    "        image = Image.open(record.image).convert(\"RGB\")\n",
    "        messages: List[Dict[str, Any]] = []\n",
    "        if record.system:\n",
    "            messages.append({\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": record.system}]})\n",
    "        user_content: List[Dict[str, Any]] = [{\"type\": \"image\", \"image\": image}]\n",
    "        if record.question:\n",
    "            user_content.append({\"type\": \"text\", \"text\": record.question})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": record.answer}]})\n",
    "        prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        return {\"prompt\": prompt, \"image\": image}\n",
    "\n",
    "class QwenDataCollator:\n",
    "    \"\"\"自定义 collator，将批次中的文本和图片一起编码为张量。\"\"\"\n",
    "    def __init__(self, processor: AutoProcessor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        prompts = [feature[\"prompt\"] for feature in features]\n",
    "        images = [feature[\"image\"] for feature in features]\n",
    "        batch = self.processor(\n",
    "            text=prompts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "        return batch\n",
    "\n",
    "def create_model(args: argparse.Namespace) -> AutoModelForVision2Seq:\n",
    "    \"\"\"加载基础模型并应用 LoRA 配置，同时启用 4bit 量化以节省显存。\"\"\"\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        args.model_name,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16 if args.bf16 else torch.float16,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    target_modules = [module.strip() for module in args.target_modules.split(\",\") if module]\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    processor = AutoProcessor.from_pretrained(args.model_name, trust_remote_code=args.trust_remote_code)\n",
    "    train_records = load_records(args.dataset_dir, args.train_file)\n",
    "    train_dataset = QwenVLDataset(train_records, args.dataset_dir, processor)\n",
    "    eval_dataset = None\n",
    "    if args.validation_file:\n",
    "        eval_records = load_records(args.dataset_dir, args.validation_file)\n",
    "        eval_dataset = QwenVLDataset(eval_records, args.dataset_dir, processor)\n",
    "    model = create_model(args)\n",
    "    collator = QwenDataCollator(processor)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        logging_steps=args.logging_steps,\n",
    "        save_strategy=args.save_strategy,\n",
    "        eval_strategy=\"no\" if eval_dataset is None else args.eval_strategy,\n",
    "        bf16=args.bf16 and torch.cuda.is_available(),\n",
    "        dataloader_num_workers=4,\n",
    "        report_to=[args.report_to] if args.report_to and args.report_to != \"none\" else [],\n",
    "        run_name=os.getenv(\"AML_RUN_ID\", \"qwen-vl-lora\"),\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    trainer.train()\n",
    "    if eval_dataset is not None:\n",
    "        metrics = trainer.evaluate()\n",
    "        logger.info(\"Evaluation metrics: %s\", metrics)\n",
    "    trainer.save_model(args.output_dir)\n",
    "    processor.save_pretrained(os.path.join(args.output_dir, \"processor\"))\n",
    "    logger.info(\"Training completed. Artifacts saved to %s\", args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065aad3e",
   "metadata": {},
   "source": [
    "## 配置训练作业\n",
    "\n",
    "定义训练数据位置、输出目录、计算集群等参数。若数据存放在默认数据存储中，可直接使用 `azureml://datastores/<datastore>/paths/...` URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb80b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# 将数据路径替换为前一节注册成功的数据资产 ID 或数据存储 URI\n",
    "# 例如：DATASET_PATH = registered_data.id\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# 使用注册的数据资产路径（如果前面已经执行了数据注册步骤）\n",
    "# 或者使用默认数据存储路径\n",
    "try:\n",
    "    # 优先使用已注册的数据资产\n",
    "    DATASET_PATH = registered_data.path\n",
    "except NameError:\n",
    "    # 如果没有注册数据资产，使用默认数据存储路径\n",
    "    DATASET_PATH = \"azureml://datastores/workspaceblobstore/paths/qwen-vl-dataset/\"\n",
    "\n",
    "TRAIN_FILE = \"train.jsonl\"\n",
    "VALIDATION_FILE = None  # 若无验证数据可设置为 None\n",
    "\n",
    "# 输出路径不需要预先创建，Azure ML 会自动处理\n",
    "# 使用默认数据存储 workspaceblobstore\n",
    "OUTPUT_PATH = f\"azureml://datastores/workspaceblobstore/paths/qwen-vl-outputs/{ml_client.workspace_name}\"\n",
    "\n",
    "# COMPUTE_NAME 已在前面的计算资源准备单元格中定义\n",
    "EXPERIMENT_NAME = \"qwen-vl-lora-demo-03\"\n",
    "\n",
    "train_input = Input(type=AssetTypes.URI_FOLDER, path=DATASET_PATH)\n",
    "\n",
    "print(f\"配置摘要:\")\n",
    "print(f\"  - 基础模型: {BASE_MODEL}\")\n",
    "print(f\"  - 数据集路径: {DATASET_PATH}\")\n",
    "print(f\"  - 训练文件: {TRAIN_FILE}\")\n",
    "print(f\"  - 验证文件: {VALIDATION_FILE}\")\n",
    "print(f\"  - 输出路径: {OUTPUT_PATH}\")\n",
    "print(f\"  - 计算集群: {COMPUTE_NAME}\")\n",
    "print(f\"  - 实验名称: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0cf1a",
   "metadata": {},
   "source": [
    "## 创建并提交命令作业\n",
    "\n",
    "在运行下方代码前请确认：\n",
    "1. `DATASET_PATH`、`OUTPUT_PATH`、`COMPUTE_NAME` 等变量已替换为实际值；\n",
    "2. 训练脚本 `train_lora.py` 已根据需要调整超参数；\n",
    "3. 若要启用 WandB/MLflow 记录，请将 `--report-to` 设置为对应后端并配置凭据。\n",
    "\n",
    "随后执行代码即可提交 Azure ML 命令作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "# 根据是否存在验证集动态拼接训练脚本命令\n",
    "validation_arg = f\"--validation-file {VALIDATION_FILE}\" if VALIDATION_FILE else \"\"\n",
    "command_parts = [\n",
    "    \"python train_lora.py\",\n",
    "    f\"--model-name {BASE_MODEL}\",\n",
    "    \"--dataset-dir ${inputs.data}\",\n",
    "    f\"--train-file {TRAIN_FILE}\",\n",
    "    \"--output-dir ./outputs\",\n",
    "    \"--per-device-train-batch-size 1\",\n",
    "    \"--gradient-accumulation-steps 8\",\n",
    "    \"--num-train-epochs 3\",\n",
    "    \"--learning-rate 2e-4\",\n",
    "    \"--logging-steps 5\"\n",
    "]\n",
    "if validation_arg:\n",
    "    command_parts.append(validation_arg)\n",
    "lora_command = \" \".join(command_parts)\n",
    "\n",
    "# 创建 Azure ML 命令作业,挂载数据并保存 LoRA 结果到指定目录\n",
    "command_job = command(\n",
    "    code=str(src_dir),\n",
    "    command=lora_command,\n",
    "    inputs={\"data\": train_input},\n",
    "    environment=registered_env,\n",
    "    compute=COMPUTE_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    display_name=\"qwen-vl-lora-train-demo-01\",\n",
    "    outputs={\n",
    "        \"trained_lora\": Output(type=AssetTypes.URI_FOLDER, path=OUTPUT_PATH)\n",
    "    },\n",
    "    description=\"LoRA finetuning job for Qwen-VL\"\n",
    ")\n",
    "\n",
    "command_job.inputs[\"data\"].mode = \"mount\"\n",
    "submitted_job = ml_client.jobs.create_or_update(command_job)\n",
    "print(f\"Job submitted: {submitted_job.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec429b9",
   "metadata": {},
   "source": [
    "## 监控训练日志\n",
    "\n",
    "运行以下代码实时查看作业日志，或直接在 Azure ML Studio 门户中监控。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39777670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在命令行实时查看训练过程输出\n",
    "ml_client.jobs.stream(submitted_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74edfe3",
   "metadata": {},
   "source": [
    "## 注册 LoRA 适配器模型\n",
    "\n",
    "作业完成后，可将输出目录注册为模型资产，方便后续部署或批量推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import ModelType\n",
    "\n",
    "# 首先获取已完成作业的详细信息\n",
    "completed_job = ml_client.jobs.get(\"serene_gas_4q7fphz2pz\")\n",
    "\n",
    "# 从已完成的作业中获取输出路径\n",
    "output_path = None\n",
    "if completed_job.outputs and \"trained_lora\" in completed_job.outputs:\n",
    "    # 尝试不同的方式获取输出路径\n",
    "    output_obj = completed_job.outputs[\"trained_lora\"]\n",
    "    if hasattr(output_obj, 'path'):\n",
    "        output_path = output_obj.path\n",
    "    elif hasattr(output_obj, 'uri'):\n",
    "        output_path = output_obj.uri\n",
    "    else:\n",
    "        # 如果上述方法都不行，构造输出路径\n",
    "        output_path = f\"{OUTPUT_PATH}/{submitted_job.name}/trained_lora\"\n",
    "\n",
    "if not output_path:\n",
    "    # 如果仍然无法获取路径，手动构造\n",
    "    output_path = f\"{OUTPUT_PATH}/{submitted_job.name}/trained_lora\"\n",
    "\n",
    "print(f\"Using output path: {output_path}\")\n",
    "\n",
    "# 将 LoRA 输出路径注册为模型资产，便于后续部署或版本管理\n",
    "# 注意：使用 ModelType.CUSTOM_MODEL 而不是 AssetTypes.URI_FOLDER\n",
    "model_asset = Model(\n",
    "    name=\"qwen-vl-lora-adapter\",\n",
    "    path=output_path,\n",
    "    type=\"custom_model\",  # 修改为支持的模型类型\n",
    "    description=\"LoRA adapter fine-tuned from Qwen-VL\",\n",
    "    tags={\"base_model\": BASE_MODEL, \"task\": \"instruction-following\"}\n",
    ")\n",
    "registered_model = ml_client.models.create_or_update(model_asset)\n",
    "print(f\"Model registered: {registered_model.name}:{registered_model.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d908e",
   "metadata": {},
   "source": [
    "## 本地验证 LoRA 效果\n",
    "\n",
    "以下示例展示如何下载输出、合并 LoRA 权重并运行简单推理。请确保有可用 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "# 下载 Azure ML 作业输出，获取存放 LoRA 权重的本地临时目录\n",
    "download_dir = pathlib.Path(tempfile.mkdtemp(prefix=\"qwen-lora-\"))\n",
    "ml_client.jobs.download(\"serene_gas_4q7fphz2pz\", output_name=\"trained_lora\", download_path=download_dir.as_posix())\n",
    "print(f\"Artifacts downloaded to {download_dir}\")\n",
    "\n",
    "# 加载基础模型并应用 LoRA 适配器\n",
    "adapter_path = \"/tmp/qwen-all-o2r7gmxz/artifacts/outputs/checkpoint-162\"\n",
    "\n",
    "# 使用具体的 Qwen2VLForConditionalGeneration 类而不是 AutoModelForVision2Seq\n",
    "try:\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Base model loaded successfully!\")\n",
    "    \n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path, trust_remote_code=True)\n",
    "    print(\"LoRA adapter loaded successfully!\")\n",
    "    \n",
    "    print(\"Loading processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "    print(\"Processor loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    # 备用方案：直接使用 AutoModel\n",
    "    from transformers import AutoModel\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path, trust_remote_code=True)\n",
    "    processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# 准备用于测试的单张图片，构造对话式输入模板\n",
    "test_image_path = \"./dataset/images/sample_000000.png\"  # 替换为实际的本地图像路径\n",
    "if not pathlib.Path(test_image_path).exists():\n",
    "    print(f\"Warning: Test image not found at {test_image_path}\")\n",
    "    print(\"Please update test_image_path to point to a valid image file\")\n",
    "else:\n",
    "    test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": test_image}, {\"type\": \"text\", \"text\": \"请描述这张图片。\"}]}\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\")\n",
    "    \n",
    "    # 如果有 GPU 可用，将输入移动到 GPU\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    # 生成回复并打印，便于快速验证效果\n",
    "    print(\"Generating response...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
    "    \n",
    "    # 解码并打印响应\n",
    "    response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Model Response:\")\n",
    "    print(\"=\"*50)\n",
    "    print(response)\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
