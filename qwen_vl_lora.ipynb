{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8485df37",
   "metadata": {},
   "source": [
    "# åœ¨ Azure Machine Learning ä¸Šä½¿ç”¨ LoRA å¾®è°ƒ Qwen-VL\n",
    "\n",
    "æœ¬ç¬”è®°æ¼”ç¤ºå¦‚ä½•åœ¨ Azure Machine Learning (Azure ML) ä¸Šå¯¹ Qwen-VL ç³»åˆ—å¤šæ¨¡æ€å¤§æ¨¡å‹æ‰§è¡Œ LoRA æŒ‡ä»¤å¾®è°ƒï¼Œå¹¶åœ¨è®­ç»ƒå®Œæˆåæ³¨å†Œæ¨¡å‹åŠéªŒè¯æ¨ç†ç»“æœã€‚æµç¨‹æ¶µç›–ç¯å¢ƒé…ç½®ã€æ•°æ®å‡†å¤‡ã€è®­ç»ƒè„šæœ¬ç¼–å†™ã€ä½œä¸šæäº¤ä¸ç›‘æ§ç­‰å…³é”®æ­¥éª¤ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373c68b",
   "metadata": {},
   "source": [
    "## å‰ææ¡ä»¶\n",
    "\n",
    "- Azure è®¢é˜…ä»¥åŠå·²åˆ›å»ºçš„ Azure ML å·¥ä½œåŒºã€‚\n",
    "- å·¥ä½œåŒºæ‰€åœ¨åŒºåŸŸå·²ç»ç”³è¯·åˆ°è¶³å¤Ÿçš„ GPU é…é¢ï¼ˆæ¨è `Standard_NC24ads_A100_v4`ï¼‰ã€‚\n",
    "- æœ¬åœ°æˆ–è®¡ç®—å®ä¾‹å·²å®‰è£… `azure-ai-ml>=1.15.0` ä¸ `azure-identity>=1.16.0`ã€‚\n",
    "- è‹¥ä½¿ç”¨æ‰˜ç®¡èº«ä»½æˆ–æœåŠ¡ä¸»ä½“ï¼Œè¯·ç¡®ä¿å…·å¤‡å¯¹å·¥ä½œåŒºçš„è®¿é—®æƒé™ã€‚\n",
    "- å‡†å¤‡å›¾æ–‡å¯¹è®­ç»ƒæ•°æ®ï¼Œæ¨èæ•´ç†ä¸º JSON Lines æ ¼å¼ï¼ˆç¤ºä¾‹è§åæ–‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d582b",
   "metadata": {},
   "source": [
    "## æ“ä½œæ–‡æ¡£æ¦‚è§ˆ\n",
    "\n",
    "æ‰§è¡Œæœ¬æŒ‡å—æ—¶å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå®Œæˆï¼š\n",
    "1. åœ¨æœ¬åœ°æ•´ç†æ•°æ®ï¼ˆå›¾ç‰‡ + æ ‡æ³¨ JSONLï¼‰ï¼Œå¹¶æ ¹æ®ç¤ºä¾‹æ£€æŸ¥å­—æ®µã€‚\n",
    "2. å°†æ•°æ®ä¸Šä¼ /æ³¨å†Œåˆ° Azure ML æ•°æ®å­˜å‚¨æˆ–æ•°æ®èµ„äº§ã€‚\n",
    "3. æ£€æŸ¥æˆ–åˆ›å»º GPU è®¡ç®—é›†ç¾¤ã€‚\n",
    "4. è¿è¡Œ Notebook å‰åŠéƒ¨åˆ†ç”Ÿæˆå¹¶æ³¨å†Œè®­ç»ƒç¯å¢ƒã€ä¸Šä¼ è„šæœ¬ã€‚\n",
    "5. æ ¹æ®å®é™…èµ„æºä¿®æ”¹å‘½ä»¤ä½œä¸šå‚æ•°å¹¶æäº¤è®­ç»ƒã€‚\n",
    "6. ç›‘æ§æ—¥å¿—ã€ä¸‹è½½è¾“å‡ºï¼Œæœ€åæ³¨å†Œæ¨¡å‹å¹¶æœ¬åœ°éªŒè¯ã€‚\n",
    "7. ï¼ˆå¯é€‰ï¼‰åŸºäºæ³¨å†Œæ¨¡å‹ç»§ç»­éƒ¨ç½²æˆ–æ‰¹é‡æ¨ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ab1b6",
   "metadata": {},
   "source": [
    "## è¿æ¥åˆ° Azure ML å·¥ä½œåŒº\n",
    "\n",
    "å¡«å†™è®¢é˜… IDã€èµ„æºç»„ä¸å·¥ä½œåŒºåç§°ã€‚ä»¥ä¸‹ç¤ºä¾‹ä¼˜å…ˆä½¿ç”¨ `DefaultAzureCredential`ï¼Œè‹¥æœ¬åœ°æ— å¯ç”¨èº«ä»½åˆ™å›é€€åˆ°äº¤äº’å¼æµè§ˆå™¨ç™»å½•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6815208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to workspace: aml-hu-east-west-us2\n",
      "Location: westus2\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "import os\n",
    "\n",
    "# è¯»å–å·¥ä½œåŒºåŸºæœ¬ä¿¡æ¯ï¼Œä¼˜å…ˆä»ç¯å¢ƒå˜é‡ä¸­è·å–\n",
    "SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\", \"7a03e9b8-18d6-48e7-b186-0ec68da9e86f\")\n",
    "RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\", \"aml-rg\")\n",
    "WORKSPACE_NAME = os.getenv(\"AZUREML_WORKSPACE_NAME\", \"aml-hu-east-west-us2\")\n",
    "\n",
    "# äº¤äº’å¼ç™»å½•\n",
    "# credential = InteractiveBrowserCredential(tenant_id=\"16b3c013-d300-468d-ac64-7eda0820b6d3\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# åˆå§‹åŒ– MLClient ä»¥ä¾¿åç»­æ“ä½œå·¥ä½œåŒºèµ„æº\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    " )\n",
    "\n",
    "print(f\"Connected to workspace: {ml_client.workspace_name}\")\n",
    "workspace = ml_client.workspaces.get(ml_client.workspace_name)  # è¯»å–å·¥ä½œåŒºä»¥è·å–åŒºåŸŸä¿¡æ¯\n",
    "print(f\"Location: {workspace.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaee20e",
   "metadata": {},
   "source": [
    "## å‡†å¤‡ç›®å½•ç»“æ„ä¸ç¯å¢ƒæ–‡ä»¶\n",
    "\n",
    "åœ¨å·¥ä½œç›®å½•ä¸‹åˆ›å»ºæºç ä¸ç¯å¢ƒé…ç½®æ–‡ä»¶å¤¹ï¼Œå¹¶å†™å…¥å°†ç”¨äºè®­ç»ƒä½œä¸šçš„ Conda ä¾èµ–ã€‚ç¯å¢ƒåŸºäº Azure ML å®˜æ–¹ PyTorch CUDA 12.1 é•œåƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8903c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source directory: /home/azureuser/qwen_vl_lora/qwen_vl_lora/src\n",
      "Environment directory: /home/azureuser/qwen_vl_lora/qwen_vl_lora/env\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# åœ¨æœ¬åœ°æ„å»ºæºç å’Œç¯å¢ƒç›®å½•ï¼Œç”¨äºç»„ç»‡è®­ç»ƒè„šæœ¬ä¸ä¾èµ–æ–‡ä»¶\n",
    "workspace_dir = pathlib.Path.cwd() / \"qwen_vl_lora\"\n",
    "src_dir = workspace_dir / \"src\"\n",
    "env_dir = workspace_dir / \"env\"\n",
    "workspace_dir.mkdir(exist_ok=True)\n",
    "src_dir.mkdir(parents=True, exist_ok=True)\n",
    "env_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Source directory: {src_dir}\")\n",
    "print(f\"Environment directory: {env_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b0a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/azureuser/qwen_vl_lora/qwen_vl_lora/env/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {env_dir}/conda.yaml\n",
    "# Conda ç¯å¢ƒæè¿°ï¼ŒAzure ML ä¼šæ®æ­¤åœ¨åŸºç¡€é•œåƒä¸Šå®‰è£…é¢å¤–ä¾èµ–\n",
    "name: qwen-vl-lora-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - accelerate>=0.28.0\n",
    "    - bitsandbytes>=0.43.0\n",
    "    - datasets>=2.18.0\n",
    "    - peft>=0.11.0\n",
    "    - pillow>=10.2.0\n",
    "    - sentencepiece>=0.2.0\n",
    "    - timm>=0.9.12\n",
    "    - torch>=2.3.0\n",
    "    - torchvision>=0.18.0\n",
    "    - transformers>=4.39.0\n",
    "    - trl>=0.8.6\n",
    "    - wandb>=0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "463bb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment registered: qwen-vl-lora-env-demo-03:1\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "# å°†è‡ªå®šä¹‰ Conda ä¾èµ–ä¸å®˜æ–¹åŸºç¡€é•œåƒç»„åˆæˆ Azure ML ç¯å¢ƒ\n",
    "# ä½¿ç”¨ HuggingFace NLP ä¸“ç”¨é•œåƒï¼ˆæ¨èç”¨äº Transformers + LoRA å¾®è°ƒï¼‰\n",
    "qwen_env = Environment(\n",
    "    name=\"qwen-vl-lora-env-demo-03\",\n",
    "    description=\"LoRA finetuning environment for Qwen-VL demo 03\",\n",
    "    conda_file=str((env_dir / \"conda.yaml\").as_posix()),\n",
    "    # æ–¹æ¡ˆ 1ï¼šHuggingFace NLP GPU ä¸“ç”¨é•œåƒï¼ˆæ¨èï¼‰\n",
    "    # å·²é¢„è£… transformers, CUDA 12.x, é’ˆå¯¹ A100 ä¼˜åŒ–\n",
    "    image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
    ")\n",
    "registered_env = ml_client.environments.create_or_update(qwen_env)\n",
    "print(f\"Environment registered: {registered_env.name}:{registered_env.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd411fe",
   "metadata": {},
   "source": [
    "### æ³¨å†Œè®­ç»ƒç¯å¢ƒ\n",
    "\n",
    "åˆ›å»ºå¹¶æ³¨å†ŒåŒ…å«æ‰€éœ€ä¾èµ–çš„ Azure ML ç¯å¢ƒã€‚æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„åŸºç¡€é•œåƒã€‚\n",
    "\n",
    "#### ğŸ¯ é•œåƒé€‰æ‹©å»ºè®®\n",
    "\n",
    "**å½“å‰é…ç½®ï¼ˆæ¨èï¼‰ï¼šHuggingFace NLP GPU ä¸“ç”¨é•œåƒ**\n",
    "```python\n",
    "image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\"\n",
    "```\n",
    "âœ… **ä¼˜ç‚¹ï¼š**\n",
    "- ä¸“é—¨ä¸º HuggingFace Transformers ä¼˜åŒ–\n",
    "- é¢„è£… CUDA 12.xï¼Œå®Œç¾æ”¯æŒæœ€æ–°çš„ bitsandbytes\n",
    "- é’ˆå¯¹ A100 GPU ä¼˜åŒ–\n",
    "- åŒ…å« NLP å’Œå¤šæ¨¡æ€ä»»åŠ¡çš„å¸¸ç”¨åº“\n",
    "\n",
    "**å…¶ä»–å¯ç”¨é€‰é¡¹ï¼š**\n",
    "\n",
    "| é•œåƒ | CUDA | é€‚ç”¨åœºæ™¯ | æ¨èåº¦ |\n",
    "|------|------|---------|--------|\n",
    "| `acft-hf-nlp-gpu:latest` | 12.x | HuggingFace + LoRA å¾®è°ƒ | â­â­â­â­â­ |\n",
    "| `pytorch-2.0-cuda11.7-cudnn8-ubuntu22.04:latest` | 11.7 | é€šç”¨ PyTorch è®­ç»ƒ | â­â­â­ |\n",
    "| `openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04:latest` | 11.8 | åˆ†å¸ƒå¼è®­ç»ƒ | â­â­â­ |\n",
    "\n",
    "#### âš ï¸ é‡è¦æç¤º\n",
    "- **bitsandbytes 0.43.0+** éœ€è¦ CUDA 12.x ä»¥è·å¾—æœ€ä½³æ€§èƒ½\n",
    "- **Qwen-VL å¤šæ¨¡æ€æ¨¡å‹** åœ¨ HuggingFace é•œåƒä¸Šè¿è¡Œæ›´ç¨³å®š\n",
    "- **A100 GPU** åœ¨ CUDA 12.x ä¸Šæ€§èƒ½æ›´ä¼˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314ef2f",
   "metadata": {},
   "source": [
    "### è®¡ç®—èµ„æºå‡†å¤‡\n",
    "1. æ¨èä½¿ç”¨å¸¦ A100 80GB çš„ GPUï¼ˆå¦‚ `Standard_NC24ads_A100_v4`ï¼‰ã€‚\n",
    "2. è‹¥å°šæœªåˆ›å»ºè®¡ç®—é›†ç¾¤ï¼Œå¯åœ¨ Azure ML Studio â†’ è®¡ç®— â†’ è®¡ç®—é›†ç¾¤ åˆ›å»ºï¼Œæˆ–ä½¿ç”¨ Azure CLIï¼š\n",
    "   ```bash\n",
    "   az ml compute create --name gpu-a100-cluster --type amlcompute \\\n",
    "       --resource-group <RG> --workspace-name <WS> \\\n",
    "       --min-instances 0 --max-instances 2 --size Standard_NC24ads_A100_v4\n",
    "   ```\n",
    "3. è®­ç»ƒå‰ç¡®è®¤é›†ç¾¤å¤„äºâ€œç©ºé—²â€æˆ–â€œå·²åˆ†é…â€çŠ¶æ€ï¼Œç¡®ä¿é…é¢è¶³å¤Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91fcd393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç°æœ‰è®¡ç®—èµ„æº:\n",
      "  - A100-swedCentral (ç±»å‹: virtualmachine, å¤§å°: N/A, çŠ¶æ€: Succeeded)\n",
      "  - A100-centre-us (ç±»å‹: virtualmachine, å¤§å°: N/A, çŠ¶æ€: Succeeded)\n",
      "  - multi-model-batch-vm (ç±»å‹: computeinstance, å¤§å°: Standard_D96ads_v5, çŠ¶æ€: Succeeded)\n",
      "  - o1-performance-test-vm (ç±»å‹: computeinstance, å¤§å°: Standard_E4ds_v4, çŠ¶æ€: Succeeded)\n",
      "  - slm-fine-tune-lab (ç±»å‹: computeinstance, å¤§å°: Standard_E4ds_v4, çŠ¶æ€: Succeeded)\n",
      "  - gpu-cluster-a100 (ç±»å‹: amlcompute, å¤§å°: Standard_NC24ads_A100_v4, çŠ¶æ€: Succeeded)\n",
      "  - agent-demo-ws (ç±»å‹: computeinstance, å¤§å°: Standard_D32d_v4, çŠ¶æ€: Succeeded)\n",
      "  - notebook-llm-solu-vm (ç±»å‹: computeinstance, å¤§å°: Standard_D48a_v4, çŠ¶æ€: Succeeded)\n",
      "  - gpu-phi4-predict-out (ç±»å‹: computeinstance, å¤§å°: Standard_NC8as_T4_v3, çŠ¶æ€: Succeeded)\n",
      "  - Standard-NV12s-v3-m60 (ç±»å‹: computeinstance, å¤§å°: Standard_NV12s_v3, çŠ¶æ€: Succeeded)\n",
      "  - multi-gpus-trainer (ç±»å‹: computeinstance, å¤§å°: Standard_NC64as_T4_v3, çŠ¶æ€: Succeeded)\n",
      "  - a100-west-1 (ç±»å‹: amlcompute, å¤§å°: Standard_NC24ads_A100_v4, çŠ¶æ€: Succeeded)\n",
      "  - qwen-fine-tune-A100 (ç±»å‹: computeinstance, å¤§å°: Standard_NC48ads_A100_v4, çŠ¶æ€: Succeeded)\n",
      "  - qwen-fine-tune-H100 (ç±»å‹: amlcompute, å¤§å°: Standard_NC80adis_H100_v5, çŠ¶æ€: Succeeded)\n",
      "\n",
      "è®¡ç®—é›†ç¾¤ 'qwen-fine-tune-H100' å·²å­˜åœ¨ï¼ŒçŠ¶æ€: Succeeded\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥ç°æœ‰è®¡ç®—èµ„æºå¹¶åœ¨éœ€è¦æ—¶åˆ›å»º GPU é›†ç¾¤\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# åˆ—å‡ºæ‰€æœ‰ç°æœ‰çš„è®¡ç®—èµ„æº\n",
    "print(\"ç°æœ‰è®¡ç®—èµ„æº:\")\n",
    "for compute in ml_client.compute.list():\n",
    "    print(f\"  - {compute.name} (ç±»å‹: {compute.type}, å¤§å°: {getattr(compute, 'size', 'N/A')}, çŠ¶æ€: {compute.provisioning_state})\")\n",
    "\n",
    "# å®šä¹‰è®¡ç®—é›†ç¾¤åç§°\n",
    "COMPUTE_NAME = \"qwen-fine-tune-H100\"\n",
    "\n",
    "# æ£€æŸ¥è®¡ç®—é›†ç¾¤æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º\n",
    "try:\n",
    "    compute_target = ml_client.compute.get(COMPUTE_NAME)\n",
    "    print(f\"\\nè®¡ç®—é›†ç¾¤ '{COMPUTE_NAME}' å·²å­˜åœ¨ï¼ŒçŠ¶æ€: {compute_target.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nè®¡ç®—é›†ç¾¤ '{COMPUTE_NAME}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...\")\n",
    "    \n",
    "    # åˆ›å»ºè®¡ç®—é›†ç¾¤é…ç½®\n",
    "    # æ³¨æ„ï¼šStandard_NC24ads_A100_v4 åœ¨æŸäº›åŒºåŸŸå¯èƒ½ä¸å¯ç”¨\n",
    "    # å¯æ›¿æ¢ä¸ºå…¶ä»– GPU SKUï¼Œå¦‚ Standard_NC6s_v3, Standard_NC12s_v3 ç­‰\n",
    "    compute_config = AmlCompute(\n",
    "        name=COMPUTE_NAME,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_NC80adis_H100_v5\",  # A100 80GB GPU\n",
    "        min_instances=0,\n",
    "        max_instances=1,\n",
    "        idle_time_before_scale_down=300,  # 5åˆ†é’Ÿåè‡ªåŠ¨ç¼©å‡\n",
    "        tier=\"dedicated\",\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        compute_target = ml_client.compute.begin_create_or_update(compute_config).result()\n",
    "        print(f\"è®¡ç®—é›†ç¾¤ '{COMPUTE_NAME}' åˆ›å»ºæˆåŠŸï¼\")\n",
    "    except Exception as create_error:\n",
    "        print(f\"åˆ›å»ºå¤±è´¥: {create_error}\")\n",
    "        print(\"\\nå¯èƒ½çš„åŸå› :\")\n",
    "        print(\"1. æ‰€é€‰ GPU SKU åœ¨å½“å‰åŒºåŸŸä¸å¯ç”¨\")\n",
    "        print(\"2. GPU é…é¢ä¸è¶³\")\n",
    "        print(\"\\nå»ºè®®:\")\n",
    "        print(\"- åœ¨ Azure Portal ä¸­æ£€æŸ¥å¯ç”¨çš„ VM SKU\")\n",
    "        print(\"- è¯·æ±‚å¢åŠ  GPU é…é¢\")\n",
    "        print(\"- æˆ–ä½¿ç”¨ç°æœ‰çš„è®¡ç®—èµ„æºï¼ˆå¦‚æœä¸Šé¢åˆ—å‡ºäº†å¯ç”¨çš„è®¡ç®—ï¼‰\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699f1d9",
   "metadata": {},
   "source": [
    "### æ£€æŸ¥å¹¶åˆ›å»ºè®¡ç®—é›†ç¾¤\n",
    "\n",
    "è¿è¡Œä¸‹æ–¹ä»£ç å•å…ƒæ ¼å°†ï¼š\n",
    "1. åˆ—å‡ºå·¥ä½œåŒºä¸­æ‰€æœ‰ç°æœ‰çš„è®¡ç®—èµ„æº\n",
    "2. æ£€æŸ¥ `gpu-a100-cluster` æ˜¯å¦å­˜åœ¨\n",
    "3. å¦‚æœä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºè¯¥é›†ç¾¤\n",
    "\n",
    "**æ³¨æ„äº‹é¡¹ï¼š**\n",
    "- å¦‚æœåˆ›å»ºå¤±è´¥ï¼ˆé…é¢ä¸è¶³æˆ– SKU ä¸å¯ç”¨ï¼‰ï¼Œå¯ä»¥ï¼š\n",
    "  - åœ¨ Azure Portal ç”³è¯· GPU é…é¢\n",
    "  - ä¿®æ”¹ä»£ç ä½¿ç”¨å…¶ä»–å¯ç”¨çš„ GPU SKUï¼ˆå¦‚ `Standard_NC6s_v3`ï¼‰\n",
    "  - æˆ–ä½¿ç”¨ä¸Šé¢åˆ—å‡ºçš„ç°æœ‰è®¡ç®—èµ„æº"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c355894",
   "metadata": {},
   "source": [
    "## å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "> ç›®æ ‡ï¼šæ•´ç†å›¾æ–‡å¤šè½®å¯¹è¯æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸º LoRA è®­ç»ƒæ‰€éœ€çš„ JSON Lines æ–‡ä»¶ã€‚\n",
    "\n",
    "### 1. æœ¬åœ°ç›®å½•ç»“æ„å»ºè®®\n",
    "- `dataset/`\bæ ¹ç›®å½•\n",
    "  - `images/`\bå­˜æ”¾æ‰€æœ‰è®­ç»ƒå›¾ç‰‡ï¼Œå¯æŒ‰éœ€è¦å†åˆ†å­ç›®å½•ã€‚\n",
    "  - `train.jsonl`\bè®­ç»ƒé›†æ ‡æ³¨æ–‡ä»¶ã€‚\n",
    "  - `validation.jsonl`\béªŒè¯é›†ï¼ˆå¯é€‰ï¼‰ã€‚\n",
    "\n",
    "### 2. JSONL å•æ¡æ ·æœ¬ç¤ºä¾‹\n",
    "```json\n",
    "{\n",
    "  \"image\": \"images/sample_0001.jpg\",\n",
    "  \"question\": \"æè¿°å›¾ç‰‡ä¸­çš„ä¸»è¦åœºæ™¯ã€‚\",\n",
    "  \"answer\": \"å›¾ç‰‡å±•ç¤ºäº†ä¸€ä½éª‘è¡Œè€…åœ¨æµ·è¾¹å…¬è·¯ä¸Šéª‘è¡Œï¼Œé˜³å…‰æ˜åªšã€‚\",\n",
    "  \"system\": \"ä½ æ˜¯ä¸€åå›¾æ–‡ç†è§£åŠ©æ‰‹ã€‚\"\n",
    "}\n",
    "```\n",
    "- `image`\bå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºæ•°æ®é›†æ ¹ç›®å½•ï¼‰æˆ–ç»å¯¹è·¯å¾„ï¼›\n",
    "- `question`\bä¸ºç”¨æˆ·æŒ‡ä»¤/æé—®ï¼›\n",
    "- `answer`\bä¸ºæ¨¡å‹æœŸæœ›å›åº”ï¼›\n",
    "- `system`\bå¯é€‰ï¼Œç”¨äºæä¾›ç³»ç»Ÿæç¤ºæˆ–è§’è‰²è®¾å®šï¼›\n",
    "- å¯æ‰©å±•é¢å¤–å­—æ®µï¼ˆå¦‚æ ‡ç­¾ã€éš¾åº¦ï¼‰ä½†éœ€åœ¨è®­ç»ƒè„šæœ¬ä¸­è‡ªè¡Œå¤„ç†ã€‚\n",
    "\n",
    "### 3. å°†å…¶ä»–æ ‡æ³¨æ ¼å¼è½¬ä¸º JSONL\n",
    "è‹¥å·²æœ‰ CSV/Excel/JSONï¼Œå¯åœ¨æœ¬åœ°è¿è¡Œä»¥ä¸‹ç¤ºä¾‹è„šæœ¬ç”Ÿæˆ `train.jsonl`ï¼š\n",
    "```python\n",
    "import csv, json, pathlib\n",
    "\n",
    "input_csv = pathlib.Path(\"./raw.csv\")\n",
    "output_jsonl = pathlib.Path(\"./dataset/train.jsonl\")\n",
    "\n",
    "with input_csv.open(\"r\", encoding=\"utf-8\") as fin, output_jsonl.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    reader = csv.DictReader(fin)\n",
    "    for row in reader:\n",
    "        record = {\n",
    "            \"image\": f\"images/{row['image_filename']}\",\n",
    "            \"question\": row[\"instruction\"],\n",
    "            \"answer\": row[\"response\"],\n",
    "            \"system\": row.get(\"system_prompt\") or \"ä½ æ˜¯ä¸€åè§†è§‰åŠ©æ‰‹ã€‚\"\n",
    "        }\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "```\n",
    "è„šæœ¬æ‰§è¡Œåç¡®è®¤ï¼š\n",
    "1. JSONL æ¯è¡Œå‡ä¸ºåˆæ³• JSONï¼›\n",
    "2. å¼•ç”¨çš„å›¾ç‰‡æ–‡ä»¶å‡å­˜åœ¨ä¸”å¯æ‰“å¼€ï¼›\n",
    "3. éªŒè¯é›†ï¼ˆè‹¥å­˜åœ¨ï¼‰ä¸è®­ç»ƒé›†ç»“æ„ä¸€è‡´å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec4705",
   "metadata": {},
   "source": [
    "è½¬æ¢å®Œæˆåé‡å¤å‰è¿°ä¸Šä¼ æ­¥éª¤ï¼Œå°†æ–°ç”Ÿæˆçš„ JSONL æ–‡ä»¶å’Œå›¾ç‰‡ç›®å½•åŒæ­¥åˆ° Azure ML æ•°æ®å­˜å‚¨å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf1eca",
   "metadata": {},
   "source": [
    "### 5. ä» Parquet/Arrow æ•°æ®æºç”Ÿæˆ JSONL\n",
    "è‹¥ä½ çš„è®­ç»ƒæ•°æ®ä¿å­˜åœ¨ `test-00000-of-00001.parquet` ç­‰ Parquet æ–‡ä»¶ä¸­ï¼Œå¯æŒ‰ä»¥ä¸‹æ­¥éª¤è½¬æ¢ï¼š\n",
    "1. ä½¿ç”¨ `datasets` æˆ– `pyarrow` è¯»å– Parquetï¼›\n",
    "2. é€è¡Œæ„å»ºæ‰€éœ€å­—æ®µå¹¶å†™å…¥ JSONLï¼›\n",
    "3. åŒæ—¶æ‰¹é‡å¯¼å‡ºå¼•ç”¨åˆ°çš„å›¾ç‰‡ï¼ˆè‹¥å­˜å‚¨ä¸ºäºŒè¿›åˆ¶æˆ–è¿œç¨‹ URLï¼‰ã€‚\n",
    "\n",
    "ä¸‹é¢ç¤ºä¾‹ Parquet ä¸­åŒ…å«å­—æ®µ 'id', 'category', 'images', 'question', 'question_text', 'answer', 'difficulty', 'metric_info', 'initial_state'ï¼Œå¹¶ä¸” `images` å·²æŒ‡å‘æœ¬åœ°å›¾ç‰‡ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e061c",
   "metadata": {},
   "source": [
    "å¦‚ Parquet ä¸­åŒ…å«åµŒå…¥äºŒè¿›åˆ¶å›¾åƒï¼Œå¯å…ˆä½¿ç”¨ `dataset[i][\"image\"].save(...)` å°†å…¶å¯¼å‡ºåˆ° `images/` ç›®å½•ï¼Œå†åœ¨ç”Ÿæˆ JSONL æ—¶å†™å…¥ç›¸å¯¹è·¯å¾„ã€‚è‹¥ `image_path` ä¸ºè¿œç¨‹ URLï¼Œéœ€æå‰ä¸‹è½½åˆ°æœ¬åœ°åå†å†™å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02885af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json, pathlib\n",
    "from PIL import Image\n",
    "\n",
    "parquet_path = \"./dataset/test-00000-of-00001.parquet\"  # ä¿®æ”¹ä¸ºä½ çš„ parquet è·¯å¾„\n",
    "output_jsonl = pathlib.Path(\"./dataset/train.jsonl\")\n",
    "images_dir = pathlib.Path(\"./dataset/images\")\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=parquet_path, split=\"train\")\n",
    "\n",
    "def save_image_as_png(image: Image.Image, image_path: pathlib.Path) -> str:\n",
    "    \"\"\"ç»Ÿä¸€å°†å›¾ç‰‡ä¿å­˜ä¸º PNGï¼Œä¿æŒ Alpha é€šé“å¹¶é¿å…å‹ç¼©æŸå¤±ã€‚\"\"\"\n",
    "    if image.mode in (\"RGBA\", \"LA\", \"P\"):\n",
    "        image = image.convert(\"RGBA\")\n",
    "    else:\n",
    "        image = image.convert(\"RGB\")\n",
    "    image.save(image_path, format=\"PNG\")\n",
    "    return f\"images/{image_path.name}\"\n",
    "\n",
    "def resolve_image_field(example, sample_idx: int) -> str:\n",
    "    \"\"\"å°† parquet ä¸­çš„ images å­—æ®µè½¬æ¢ä¸º JSONL æ‰€éœ€çš„å›¾ç‰‡è·¯å¾„ã€‚\"\"\"\n",
    "    image_field = example.get(\"images\")\n",
    "    if isinstance(image_field, list):\n",
    "        image_field = image_field[0] if image_field else None\n",
    "    if isinstance(image_field, Image.Image):\n",
    "        image_path = images_dir / f\"{sample_idx:06d}.png\"\n",
    "        return save_image_as_png(image_field, image_path)\n",
    "    if hasattr(image_field, \"save\"):\n",
    "        pil_image = image_field\n",
    "        image_path = images_dir / f\"{sample_idx:06d}.png\"\n",
    "        return save_image_as_png(pil_image, image_path)\n",
    "    if isinstance(image_field, dict):\n",
    "        candidate = image_field.get(\"path\") or image_field.get(\"image_path\") or image_field.get(\"url\")\n",
    "        if candidate:\n",
    "            return candidate\n",
    "    if isinstance(image_field, str):\n",
    "        return image_field\n",
    "    raise ValueError(\"æ— æ³•è§£æ parquet ä¸­çš„ images å­—æ®µï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼\")\n",
    "\n",
    "with output_jsonl.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, example in enumerate(dataset):\n",
    "        record = {\n",
    "            \"image\": resolve_image_field(example, idx),\n",
    "            \"question\": example.get(\"question_text\") or example.get(\"question\") or \"\",\n",
    "            \"answer\": example.get(\"answer\", \"\"),\n",
    "            \"system\": example.get(\"initial_state\") or \"ä½ æ˜¯ä¸€åè§†è§‰åŠ©æ‰‹ã€‚\",\n",
    "            \"category\": example.get(\"category\"),\n",
    "            \"metadata\": {\"difficulty\": example.get(\"difficulty\"), \"metric_info\": example.get(\"metric_info\")},\n",
    "        }\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Wrote {len(dataset)} samples to {output_jsonl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f398f5b",
   "metadata": {},
   "source": [
    "### 4. å°†æœ¬åœ°æ•°æ®ä¸Šä¼ åˆ° Azure ML\n",
    "1. **ç¡®è®¤é»˜è®¤æ•°æ®å­˜å‚¨**ï¼šè¿è¡Œä¸‹æ–¹ä»£ç æŸ¥çœ‹ `workspaceblobstore` æˆ–è‡ªå®šä¹‰æ•°æ®å­˜å‚¨çš„åç§°ã€‚\n",
    "2. **ä¸Šä¼ /æ³¨å†Œæ•°æ®èµ„äº§**ï¼šå¯ä½¿ç”¨ Notebook ä¸­çš„ Python ä»£ç ï¼Œä¹Ÿå¯é€šè¿‡ Azure ML Studio çš„â€œæ•°æ®â€é¡µé¢æ‰‹åŠ¨ä¸Šä¼ ã€‚\n",
    "3. **ä¿æŒç›®å½•ç»“æ„**ï¼šä¸Šä¼ æ—¶ç¡®ä¿ `images/` ç­‰å­ç›®å½•ä¸ JSONL æ–‡ä»¶ä¿æŒåŸæœ‰å±‚çº§ã€‚\n",
    "4. **æ•°æ®æ›´æ–°**ï¼šè‹¥é‡æ–°ä¸Šä¼ ï¼Œå¯é€‰æ‹©æ–°ç‰ˆæœ¬å·ï¼Œé¿å…è¦†ç›–å†å²æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "627a1a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml_globaldatasets -> DatastoreType.AZURE_BLOB\n",
      "workspaceblobstore -> DatastoreType.AZURE_BLOB\n",
      "workspaceartifactstore -> DatastoreType.AZURE_BLOB\n",
      "workspaceworkingdirectory -> DatastoreType.AZURE_FILE\n",
      "workspacefilestore -> DatastoreType.AZURE_FILE\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹å½“å‰å·¥ä½œåŒºå†…å¯ç”¨çš„æ•°æ®å­˜å‚¨ï¼ˆé»˜è®¤é€šå¸¸ä¸º workspaceblobstoreï¼‰\n",
    "for ds in ml_client.datastores.list():\n",
    "    print(ds.name, \"->\", ds.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ff4f0",
   "metadata": {},
   "source": [
    "> **æç¤º**ï¼šå¦‚æœæ•°æ®é‡è¾ƒå¤§ï¼Œå¯ä½¿ç”¨ `azcopy` æˆ– Azure Storage Explorer å…ˆä¸Šä¼ åˆ° Blobï¼Œå†æ³¨å†Œä¸ºæ•°æ®èµ„äº§ã€‚ä¸‹æ–¹ç¤ºä¾‹é€‚åˆç›´æ¥ä»æœ¬åœ°æ–‡ä»¶å¤¹ä¸Šä¼ ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fabb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'timezone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_dataset_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæœ¬åœ°æ•°æ®ç›®å½•ä¸å­˜åœ¨: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_dataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m data_asset \u001b[38;5;241m=\u001b[39m Data(\n\u001b[1;32m     13\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen-vl-instruction-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 14\u001b[0m     version\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimezone\u001b[49m\u001b[38;5;241m.\u001b[39mutc)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     path\u001b[38;5;241m=\u001b[39mlocal_dataset_path\u001b[38;5;241m.\u001b[39mas_posix(),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mAssetTypes\u001b[38;5;241m.\u001b[39mURI_FOLDER,\n\u001b[1;32m     17\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen-VL LoRA instruction dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m registered_data \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcreate_or_update(data_asset)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData asset created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistered_data\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistered_data\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'timezone'"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "# å°†æœ¬åœ°æ–‡ä»¶å¤¹ä¸Šä¼ å¹¶æ³¨å†Œä¸ºæ•°æ®èµ„äº§\n",
    "local_dataset_path = pathlib.Path(\"./dataset\")  # ä¿®æ”¹ä¸ºä½ çš„æœ¬åœ°æ•°æ®é›†è·¯å¾„\n",
    "if not local_dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"æœ¬åœ°æ•°æ®ç›®å½•ä¸å­˜åœ¨: {local_dataset_path}\")\n",
    "\n",
    "data_asset = Data(\n",
    "    name=\"qwen-vl-instruction-data\",\n",
    "    version=datetime.utcnow().strftime(\"%Y%m%d%H%M\"),\n",
    "    path=local_dataset_path.as_posix(),\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Qwen-VL LoRA instruction dataset\",\n",
    ")\n",
    "\n",
    "registered_data = ml_client.data.create_or_update(data_asset)\n",
    "print(f\"Data asset created: {registered_data.name}:{registered_data.version}\")\n",
    "print(f\"Asset path: {registered_data.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118ff8e",
   "metadata": {},
   "source": [
    "è¿è¡ŒæˆåŠŸåï¼Œè®°ä¸‹ `Data asset created` è¾“å‡ºçš„ ID/è·¯å¾„ï¼Œå¹¶å°†å…¶å¡«å†™åˆ°åç»­ `DATASET_PATH` å˜é‡ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {src_dir}/train_lora.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    )\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    \"\"\"è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œä¾¿äºåœ¨ Azure ML ä½œä¸šä¸­çµæ´»ä¼ å‚ã€‚\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"LoRA finetuning for Qwen-VL demo 03\")\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "    parser.add_argument(\"--dataset-dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train.jsonl\")\n",
    "    parser.add_argument(\"--validation-file\", type=str, default=None)\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"./outputs\")\n",
    "    parser.add_argument(\"--per-device-train-batch-size\", type=int, default=1)\n",
    "    parser.add_argument(\"--per-device-eval-batch-size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=8)\n",
    "    parser.add_argument(\"--num-train-epochs\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--warmup-ratio\", type=float, default=0.03)\n",
    "    parser.add_argument(\"--logging-steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--save-strategy\", type=str, default=\"epoch\")\n",
    "    parser.add_argument(\"--eval-strategy\", type=str, default=\"epoch\")\n",
    "    parser.add_argument(\"--lora-rank\", type=int, default=64)\n",
    "    parser.add_argument(\"--lora-alpha\", type=int, default=128)\n",
    "    parser.add_argument(\"--lora-dropout\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--target-modules\", type=str, default=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\")\n",
    "    parser.add_argument(\"--bf16\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--trust-remote-code\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--report-to\", type=str, default=\"none\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def load_records(dataset_dir: str, file_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"è¯»å– JSONL æ–‡ä»¶ï¼Œå¹¶è¿”å›æ ·æœ¬åˆ—è¡¨ã€‚\"\"\"\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    logger.info(\"Loaded %d samples from %s\", len(records), file_path)\n",
    "    return records\n",
    "\n",
    "def resolve_image_path(dataset_dir: str, image_path: str) -> str:\n",
    "    \"\"\"ç»Ÿä¸€å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œä¾¿äºåœ¨ Azure ML è®¡ç®—èŠ‚ç‚¹ä¸Šè®¿é—®å›¾ç‰‡ã€‚\"\"\"\n",
    "    return image_path if os.path.isabs(image_path) else os.path.join(dataset_dir, image_path)\n",
    "\n",
    "@dataclass\n",
    "class QwenRecord:\n",
    "    \"\"\"ç”¨ dataclass å­˜å‚¨å•æ¡æ ·æœ¬ï¼Œæå‡å¯è¯»æ€§ã€‚\"\"\"\n",
    "    image: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    system: Optional[str] = None\n",
    "\n",
    "class QwenVLDataset(Dataset):\n",
    "    \"\"\"å°†åŸå§‹ JSON æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç›´æ¥ä½¿ç”¨çš„æç¤ºæ ¼å¼ã€‚\"\"\"\n",
    "    def __init__(self, records: List[Dict[str, Any]], dataset_dir: str, processor: AutoProcessor):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.processor = processor\n",
    "        self.records: List[QwenRecord] = [\n",
    "            QwenRecord(\n",
    "                image=resolve_image_path(dataset_dir, item[\"image\"]),\n",
    "                question=item.get(\"question\", \"\"),\n",
    "                answer=item.get(\"answer\", \"\"),\n",
    "                system=item.get(\"system\")\n",
    "            )\n",
    "            for item in records\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        record = self.records[idx]\n",
    "        if not os.path.exists(record.image):\n",
    "            raise FileNotFoundError(f\"Image not found: {record.image}\")\n",
    "        image = Image.open(record.image).convert(\"RGB\")\n",
    "        messages: List[Dict[str, Any]] = []\n",
    "        if record.system:\n",
    "            messages.append({\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": record.system}]})\n",
    "        user_content: List[Dict[str, Any]] = [{\"type\": \"image\", \"image\": image}]\n",
    "        if record.question:\n",
    "            user_content.append({\"type\": \"text\", \"text\": record.question})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": record.answer}]})\n",
    "        prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        return {\"prompt\": prompt, \"image\": image}\n",
    "\n",
    "class QwenDataCollator:\n",
    "    \"\"\"è‡ªå®šä¹‰ collatorï¼Œå°†æ‰¹æ¬¡ä¸­çš„æ–‡æœ¬å’Œå›¾ç‰‡ä¸€èµ·ç¼–ç ä¸ºå¼ é‡ã€‚\"\"\"\n",
    "    def __init__(self, processor: AutoProcessor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        prompts = [feature[\"prompt\"] for feature in features]\n",
    "        images = [feature[\"image\"] for feature in features]\n",
    "        batch = self.processor(\n",
    "            text=prompts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "        return batch\n",
    "\n",
    "def create_model(args: argparse.Namespace) -> AutoModelForVision2Seq:\n",
    "    \"\"\"åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨ LoRA é…ç½®ï¼ŒåŒæ—¶å¯ç”¨ 4bit é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ã€‚\"\"\"\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        args.model_name,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16 if args.bf16 else torch.float16,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    target_modules = [module.strip() for module in args.target_modules.split(\",\") if module]\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    processor = AutoProcessor.from_pretrained(args.model_name, trust_remote_code=args.trust_remote_code)\n",
    "    train_records = load_records(args.dataset_dir, args.train_file)\n",
    "    train_dataset = QwenVLDataset(train_records, args.dataset_dir, processor)\n",
    "    eval_dataset = None\n",
    "    if args.validation_file:\n",
    "        eval_records = load_records(args.dataset_dir, args.validation_file)\n",
    "        eval_dataset = QwenVLDataset(eval_records, args.dataset_dir, processor)\n",
    "    model = create_model(args)\n",
    "    collator = QwenDataCollator(processor)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        logging_steps=args.logging_steps,\n",
    "        save_strategy=args.save_strategy,\n",
    "        eval_strategy=\"no\" if eval_dataset is None else args.eval_strategy,\n",
    "        bf16=args.bf16 and torch.cuda.is_available(),\n",
    "        dataloader_num_workers=4,\n",
    "        report_to=[args.report_to] if args.report_to and args.report_to != \"none\" else [],\n",
    "        run_name=os.getenv(\"AML_RUN_ID\", \"qwen-vl-lora\"),\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    trainer.train()\n",
    "    if eval_dataset is not None:\n",
    "        metrics = trainer.evaluate()\n",
    "        logger.info(\"Evaluation metrics: %s\", metrics)\n",
    "    trainer.save_model(args.output_dir)\n",
    "    processor.save_pretrained(os.path.join(args.output_dir, \"processor\"))\n",
    "    logger.info(\"Training completed. Artifacts saved to %s\", args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065aad3e",
   "metadata": {},
   "source": [
    "## é…ç½®è®­ç»ƒä½œä¸š\n",
    "\n",
    "å®šä¹‰è®­ç»ƒæ•°æ®ä½ç½®ã€è¾“å‡ºç›®å½•ã€è®¡ç®—é›†ç¾¤ç­‰å‚æ•°ã€‚è‹¥æ•°æ®å­˜æ”¾åœ¨é»˜è®¤æ•°æ®å­˜å‚¨ä¸­ï¼Œå¯ç›´æ¥ä½¿ç”¨ `azureml://datastores/<datastore>/paths/...` URIã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb80b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# å°†æ•°æ®è·¯å¾„æ›¿æ¢ä¸ºå‰ä¸€èŠ‚æ³¨å†ŒæˆåŠŸçš„æ•°æ®èµ„äº§ ID æˆ–æ•°æ®å­˜å‚¨ URI\n",
    "# ä¾‹å¦‚ï¼šDATASET_PATH = registered_data.id\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# ä½¿ç”¨æ³¨å†Œçš„æ•°æ®èµ„äº§è·¯å¾„ï¼ˆå¦‚æœå‰é¢å·²ç»æ‰§è¡Œäº†æ•°æ®æ³¨å†Œæ­¥éª¤ï¼‰\n",
    "# æˆ–è€…ä½¿ç”¨é»˜è®¤æ•°æ®å­˜å‚¨è·¯å¾„\n",
    "try:\n",
    "    # ä¼˜å…ˆä½¿ç”¨å·²æ³¨å†Œçš„æ•°æ®èµ„äº§\n",
    "    DATASET_PATH = registered_data.path\n",
    "except NameError:\n",
    "    # å¦‚æœæ²¡æœ‰æ³¨å†Œæ•°æ®èµ„äº§ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®å­˜å‚¨è·¯å¾„\n",
    "    DATASET_PATH = \"azureml://datastores/workspaceblobstore/paths/qwen-vl-dataset/\"\n",
    "\n",
    "TRAIN_FILE = \"train.jsonl\"\n",
    "VALIDATION_FILE = None  # è‹¥æ— éªŒè¯æ•°æ®å¯è®¾ç½®ä¸º None\n",
    "\n",
    "# è¾“å‡ºè·¯å¾„ä¸éœ€è¦é¢„å…ˆåˆ›å»ºï¼ŒAzure ML ä¼šè‡ªåŠ¨å¤„ç†\n",
    "# ä½¿ç”¨é»˜è®¤æ•°æ®å­˜å‚¨ workspaceblobstore\n",
    "OUTPUT_PATH = f\"azureml://datastores/workspaceblobstore/paths/qwen-vl-outputs/{ml_client.workspace_name}\"\n",
    "\n",
    "# COMPUTE_NAME å·²åœ¨å‰é¢çš„è®¡ç®—èµ„æºå‡†å¤‡å•å…ƒæ ¼ä¸­å®šä¹‰\n",
    "EXPERIMENT_NAME = \"qwen-vl-lora-demo-03\"\n",
    "\n",
    "train_input = Input(type=AssetTypes.URI_FOLDER, path=DATASET_PATH)\n",
    "\n",
    "print(f\"é…ç½®æ‘˜è¦:\")\n",
    "print(f\"  - åŸºç¡€æ¨¡å‹: {BASE_MODEL}\")\n",
    "print(f\"  - æ•°æ®é›†è·¯å¾„: {DATASET_PATH}\")\n",
    "print(f\"  - è®­ç»ƒæ–‡ä»¶: {TRAIN_FILE}\")\n",
    "print(f\"  - éªŒè¯æ–‡ä»¶: {VALIDATION_FILE}\")\n",
    "print(f\"  - è¾“å‡ºè·¯å¾„: {OUTPUT_PATH}\")\n",
    "print(f\"  - è®¡ç®—é›†ç¾¤: {COMPUTE_NAME}\")\n",
    "print(f\"  - å®éªŒåç§°: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0cf1a",
   "metadata": {},
   "source": [
    "## åˆ›å»ºå¹¶æäº¤å‘½ä»¤ä½œä¸š\n",
    "\n",
    "åœ¨è¿è¡Œä¸‹æ–¹ä»£ç å‰è¯·ç¡®è®¤ï¼š\n",
    "1. `DATASET_PATH`ã€`OUTPUT_PATH`ã€`COMPUTE_NAME` ç­‰å˜é‡å·²æ›¿æ¢ä¸ºå®é™…å€¼ï¼›\n",
    "2. è®­ç»ƒè„šæœ¬ `train_lora.py` å·²æ ¹æ®éœ€è¦è°ƒæ•´è¶…å‚æ•°ï¼›\n",
    "3. è‹¥è¦å¯ç”¨ WandB/MLflow è®°å½•ï¼Œè¯·å°† `--report-to` è®¾ç½®ä¸ºå¯¹åº”åç«¯å¹¶é…ç½®å‡­æ®ã€‚\n",
    "\n",
    "éšåæ‰§è¡Œä»£ç å³å¯æäº¤ Azure ML å‘½ä»¤ä½œä¸šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "# æ ¹æ®æ˜¯å¦å­˜åœ¨éªŒè¯é›†åŠ¨æ€æ‹¼æ¥è®­ç»ƒè„šæœ¬å‘½ä»¤\n",
    "validation_arg = f\"--validation-file {VALIDATION_FILE}\" if VALIDATION_FILE else \"\"\n",
    "command_parts = [\n",
    "    \"python train_lora.py\",\n",
    "    f\"--model-name {BASE_MODEL}\",\n",
    "    \"--dataset-dir ${inputs.data}\",\n",
    "    f\"--train-file {TRAIN_FILE}\",\n",
    "    \"--output-dir ./outputs\",\n",
    "    \"--per-device-train-batch-size 1\",\n",
    "    \"--gradient-accumulation-steps 8\",\n",
    "    \"--num-train-epochs 3\",\n",
    "    \"--learning-rate 2e-4\",\n",
    "    \"--logging-steps 5\"\n",
    "]\n",
    "if validation_arg:\n",
    "    command_parts.append(validation_arg)\n",
    "lora_command = \" \".join(command_parts)\n",
    "\n",
    "# åˆ›å»º Azure ML å‘½ä»¤ä½œä¸š,æŒ‚è½½æ•°æ®å¹¶ä¿å­˜ LoRA ç»“æœåˆ°æŒ‡å®šç›®å½•\n",
    "command_job = command(\n",
    "    code=str(src_dir),\n",
    "    command=lora_command,\n",
    "    inputs={\"data\": train_input},\n",
    "    environment=registered_env,\n",
    "    compute=COMPUTE_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    display_name=\"qwen-vl-lora-train-demo-01\",\n",
    "    outputs={\n",
    "        \"trained_lora\": Output(type=AssetTypes.URI_FOLDER, path=OUTPUT_PATH)\n",
    "    },\n",
    "    description=\"LoRA finetuning job for Qwen-VL\"\n",
    ")\n",
    "\n",
    "command_job.inputs[\"data\"].mode = \"mount\"\n",
    "submitted_job = ml_client.jobs.create_or_update(command_job)\n",
    "print(f\"Job submitted: {submitted_job.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec429b9",
   "metadata": {},
   "source": [
    "## ç›‘æ§è®­ç»ƒæ—¥å¿—\n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç å®æ—¶æŸ¥çœ‹ä½œä¸šæ—¥å¿—ï¼Œæˆ–ç›´æ¥åœ¨ Azure ML Studio é—¨æˆ·ä¸­ç›‘æ§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39777670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨å‘½ä»¤è¡Œå®æ—¶æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹è¾“å‡º\n",
    "ml_client.jobs.stream(submitted_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74edfe3",
   "metadata": {},
   "source": [
    "## æ³¨å†Œ LoRA é€‚é…å™¨æ¨¡å‹\n",
    "\n",
    "ä½œä¸šå®Œæˆåï¼Œå¯å°†è¾“å‡ºç›®å½•æ³¨å†Œä¸ºæ¨¡å‹èµ„äº§ï¼Œæ–¹ä¾¿åç»­éƒ¨ç½²æˆ–æ‰¹é‡æ¨ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import ModelType\n",
    "\n",
    "# é¦–å…ˆè·å–å·²å®Œæˆä½œä¸šçš„è¯¦ç»†ä¿¡æ¯\n",
    "completed_job = ml_client.jobs.get(\"serene_gas_4q7fphz2pz\")\n",
    "\n",
    "# ä»å·²å®Œæˆçš„ä½œä¸šä¸­è·å–è¾“å‡ºè·¯å¾„\n",
    "output_path = None\n",
    "if completed_job.outputs and \"trained_lora\" in completed_job.outputs:\n",
    "    # å°è¯•ä¸åŒçš„æ–¹å¼è·å–è¾“å‡ºè·¯å¾„\n",
    "    output_obj = completed_job.outputs[\"trained_lora\"]\n",
    "    if hasattr(output_obj, 'path'):\n",
    "        output_path = output_obj.path\n",
    "    elif hasattr(output_obj, 'uri'):\n",
    "        output_path = output_obj.uri\n",
    "    else:\n",
    "        # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½ä¸è¡Œï¼Œæ„é€ è¾“å‡ºè·¯å¾„\n",
    "        output_path = f\"{OUTPUT_PATH}/{submitted_job.name}/trained_lora\"\n",
    "\n",
    "if not output_path:\n",
    "    # å¦‚æœä»ç„¶æ— æ³•è·å–è·¯å¾„ï¼Œæ‰‹åŠ¨æ„é€ \n",
    "    output_path = f\"{OUTPUT_PATH}/{submitted_job.name}/trained_lora\"\n",
    "\n",
    "print(f\"Using output path: {output_path}\")\n",
    "\n",
    "# å°† LoRA è¾“å‡ºè·¯å¾„æ³¨å†Œä¸ºæ¨¡å‹èµ„äº§ï¼Œä¾¿äºåç»­éƒ¨ç½²æˆ–ç‰ˆæœ¬ç®¡ç†\n",
    "# æ³¨æ„ï¼šä½¿ç”¨ ModelType.CUSTOM_MODEL è€Œä¸æ˜¯ AssetTypes.URI_FOLDER\n",
    "model_asset = Model(\n",
    "    name=\"qwen-vl-lora-adapter\",\n",
    "    path=output_path,\n",
    "    type=\"custom_model\",  # ä¿®æ”¹ä¸ºæ”¯æŒçš„æ¨¡å‹ç±»å‹\n",
    "    description=\"LoRA adapter fine-tuned from Qwen-VL\",\n",
    "    tags={\"base_model\": BASE_MODEL, \"task\": \"instruction-following\"}\n",
    ")\n",
    "registered_model = ml_client.models.create_or_update(model_asset)\n",
    "print(f\"Model registered: {registered_model.name}:{registered_model.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d908e",
   "metadata": {},
   "source": [
    "## æœ¬åœ°éªŒè¯ LoRA æ•ˆæœ\n",
    "\n",
    "ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä¸‹è½½è¾“å‡ºã€åˆå¹¶ LoRA æƒé‡å¹¶è¿è¡Œç®€å•æ¨ç†ã€‚è¯·ç¡®ä¿æœ‰å¯ç”¨ GPUã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "# ä¸‹è½½ Azure ML ä½œä¸šè¾“å‡ºï¼Œè·å–å­˜æ”¾ LoRA æƒé‡çš„æœ¬åœ°ä¸´æ—¶ç›®å½•\n",
    "download_dir = pathlib.Path(tempfile.mkdtemp(prefix=\"qwen-lora-\"))\n",
    "ml_client.jobs.download(\"serene_gas_4q7fphz2pz\", output_name=\"trained_lora\", download_path=download_dir.as_posix())\n",
    "print(f\"Artifacts downloaded to {download_dir}\")\n",
    "\n",
    "# åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨ LoRA é€‚é…å™¨\n",
    "adapter_path = \"/tmp/qwen-all-o2r7gmxz/artifacts/outputs/checkpoint-162\"\n",
    "\n",
    "# ä½¿ç”¨å…·ä½“çš„ Qwen2VLForConditionalGeneration ç±»è€Œä¸æ˜¯ AutoModelForVision2Seq\n",
    "try:\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Base model loaded successfully!\")\n",
    "    \n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path, trust_remote_code=True)\n",
    "    print(\"LoRA adapter loaded successfully!\")\n",
    "    \n",
    "    print(\"Loading processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "    print(\"Processor loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨ AutoModel\n",
    "    from transformers import AutoModel\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path, trust_remote_code=True)\n",
    "    processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# å‡†å¤‡ç”¨äºæµ‹è¯•çš„å•å¼ å›¾ç‰‡ï¼Œæ„é€ å¯¹è¯å¼è¾“å…¥æ¨¡æ¿\n",
    "test_image_path = \"./dataset/images/sample_000000.png\"  # æ›¿æ¢ä¸ºå®é™…çš„æœ¬åœ°å›¾åƒè·¯å¾„\n",
    "if not pathlib.Path(test_image_path).exists():\n",
    "    print(f\"Warning: Test image not found at {test_image_path}\")\n",
    "    print(\"Please update test_image_path to point to a valid image file\")\n",
    "else:\n",
    "    test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": test_image}, {\"type\": \"text\", \"text\": \"è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚\"}]}\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\")\n",
    "    \n",
    "    # å¦‚æœæœ‰ GPU å¯ç”¨ï¼Œå°†è¾“å…¥ç§»åŠ¨åˆ° GPU\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    # ç”Ÿæˆå›å¤å¹¶æ‰“å°ï¼Œä¾¿äºå¿«é€ŸéªŒè¯æ•ˆæœ\n",
    "    print(\"Generating response...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
    "    \n",
    "    # è§£ç å¹¶æ‰“å°å“åº”\n",
    "    response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Model Response:\")\n",
    "    print(\"=\"*50)\n",
    "    print(response)\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
